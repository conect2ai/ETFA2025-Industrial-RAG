{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# **Analysis**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from scipy import stats\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "import math\n",
    "#plt\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Dev**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_base = pd.read_csv('./Validation_Results/df_base.csv')\n",
    "df_decomposition = pd.read_csv('./Validation_Results/df_decomposition.csv')\n",
    "df_hyde = pd.read_csv('./Validation_Results/df_hyde.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_gpt_base = pd.read_csv('./Validation_Results/df_gpt_base.csv')\n",
    "df_gpt_decomposition = pd.read_csv('./Validation_Results/df_gpt_decomposition.csv')\n",
    "df_gpt_hyde = pd.read_csv('./Validation_Results/df_gpt_hyde.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Concat Dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1-P</th>\n",
       "      <th>rouge1-R</th>\n",
       "      <th>rouge1-F1</th>\n",
       "      <th>rouge2-P</th>\n",
       "      <th>rouge2-R</th>\n",
       "      <th>rouge2-F1</th>\n",
       "      <th>rougeL-P</th>\n",
       "      <th>rougeL-R</th>\n",
       "      <th>rougeL-F1</th>\n",
       "      <th>METEOR</th>\n",
       "      <th>BERTScore Precision</th>\n",
       "      <th>BERTScore Recall</th>\n",
       "      <th>BERTScore F1</th>\n",
       "      <th>MATTR</th>\n",
       "      <th>model</th>\n",
       "      <th>tech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.674419</td>\n",
       "      <td>0.213235</td>\n",
       "      <td>0.324022</td>\n",
       "      <td>0.171875</td>\n",
       "      <td>0.054054</td>\n",
       "      <td>0.082243</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.117647</td>\n",
       "      <td>0.178771</td>\n",
       "      <td>0.135188</td>\n",
       "      <td>0.881479</td>\n",
       "      <td>0.852133</td>\n",
       "      <td>0.866558</td>\n",
       "      <td>0.784615</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.622222</td>\n",
       "      <td>0.062500</td>\n",
       "      <td>0.113590</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.020134</td>\n",
       "      <td>0.036660</td>\n",
       "      <td>0.355556</td>\n",
       "      <td>0.035714</td>\n",
       "      <td>0.064909</td>\n",
       "      <td>0.035565</td>\n",
       "      <td>0.882748</td>\n",
       "      <td>0.793773</td>\n",
       "      <td>0.835900</td>\n",
       "      <td>0.800000</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.646018</td>\n",
       "      <td>0.130590</td>\n",
       "      <td>0.217262</td>\n",
       "      <td>0.241071</td>\n",
       "      <td>0.048387</td>\n",
       "      <td>0.080597</td>\n",
       "      <td>0.442478</td>\n",
       "      <td>0.089445</td>\n",
       "      <td>0.148810</td>\n",
       "      <td>0.081500</td>\n",
       "      <td>0.877768</td>\n",
       "      <td>0.818323</td>\n",
       "      <td>0.847004</td>\n",
       "      <td>0.758125</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.614035</td>\n",
       "      <td>0.064457</td>\n",
       "      <td>0.116667</td>\n",
       "      <td>0.232143</td>\n",
       "      <td>0.023985</td>\n",
       "      <td>0.043478</td>\n",
       "      <td>0.473684</td>\n",
       "      <td>0.049724</td>\n",
       "      <td>0.090000</td>\n",
       "      <td>0.042399</td>\n",
       "      <td>0.883883</td>\n",
       "      <td>0.800092</td>\n",
       "      <td>0.839903</td>\n",
       "      <td>0.780000</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.743590</td>\n",
       "      <td>0.148338</td>\n",
       "      <td>0.247335</td>\n",
       "      <td>0.324675</td>\n",
       "      <td>0.064103</td>\n",
       "      <td>0.107066</td>\n",
       "      <td>0.435897</td>\n",
       "      <td>0.086957</td>\n",
       "      <td>0.144989</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.894963</td>\n",
       "      <td>0.814272</td>\n",
       "      <td>0.852713</td>\n",
       "      <td>0.713103</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.572650</td>\n",
       "      <td>0.123162</td>\n",
       "      <td>0.202723</td>\n",
       "      <td>0.206897</td>\n",
       "      <td>0.044199</td>\n",
       "      <td>0.072838</td>\n",
       "      <td>0.393162</td>\n",
       "      <td>0.084559</td>\n",
       "      <td>0.139183</td>\n",
       "      <td>0.081424</td>\n",
       "      <td>0.886397</td>\n",
       "      <td>0.837483</td>\n",
       "      <td>0.861246</td>\n",
       "      <td>0.755882</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.612732</td>\n",
       "      <td>0.292035</td>\n",
       "      <td>0.395548</td>\n",
       "      <td>0.228723</td>\n",
       "      <td>0.108861</td>\n",
       "      <td>0.147513</td>\n",
       "      <td>0.275862</td>\n",
       "      <td>0.131479</td>\n",
       "      <td>0.178082</td>\n",
       "      <td>0.185130</td>\n",
       "      <td>0.882320</td>\n",
       "      <td>0.866433</td>\n",
       "      <td>0.874305</td>\n",
       "      <td>0.778293</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.632479</td>\n",
       "      <td>0.318280</td>\n",
       "      <td>0.423462</td>\n",
       "      <td>0.171674</td>\n",
       "      <td>0.086207</td>\n",
       "      <td>0.114778</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.167742</td>\n",
       "      <td>0.223176</td>\n",
       "      <td>0.191558</td>\n",
       "      <td>0.875856</td>\n",
       "      <td>0.852887</td>\n",
       "      <td>0.864219</td>\n",
       "      <td>0.755459</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.433420</td>\n",
       "      <td>0.348008</td>\n",
       "      <td>0.386047</td>\n",
       "      <td>0.086387</td>\n",
       "      <td>0.069328</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.190601</td>\n",
       "      <td>0.153040</td>\n",
       "      <td>0.169767</td>\n",
       "      <td>0.198817</td>\n",
       "      <td>0.865793</td>\n",
       "      <td>0.856556</td>\n",
       "      <td>0.861150</td>\n",
       "      <td>0.669701</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.187755</td>\n",
       "      <td>0.292063</td>\n",
       "      <td>0.237410</td>\n",
       "      <td>0.067485</td>\n",
       "      <td>0.105096</td>\n",
       "      <td>0.378571</td>\n",
       "      <td>0.108163</td>\n",
       "      <td>0.168254</td>\n",
       "      <td>0.110851</td>\n",
       "      <td>0.878814</td>\n",
       "      <td>0.840818</td>\n",
       "      <td>0.859396</td>\n",
       "      <td>0.787033</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.646512</td>\n",
       "      <td>0.340686</td>\n",
       "      <td>0.446228</td>\n",
       "      <td>0.177570</td>\n",
       "      <td>0.093366</td>\n",
       "      <td>0.122383</td>\n",
       "      <td>0.339535</td>\n",
       "      <td>0.178922</td>\n",
       "      <td>0.234350</td>\n",
       "      <td>0.204217</td>\n",
       "      <td>0.884044</td>\n",
       "      <td>0.874819</td>\n",
       "      <td>0.879408</td>\n",
       "      <td>0.741098</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.498947</td>\n",
       "      <td>0.529018</td>\n",
       "      <td>0.513543</td>\n",
       "      <td>0.141350</td>\n",
       "      <td>0.149888</td>\n",
       "      <td>0.145494</td>\n",
       "      <td>0.204211</td>\n",
       "      <td>0.216518</td>\n",
       "      <td>0.210184</td>\n",
       "      <td>0.298219</td>\n",
       "      <td>0.871369</td>\n",
       "      <td>0.872314</td>\n",
       "      <td>0.871841</td>\n",
       "      <td>0.754178</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.577320</td>\n",
       "      <td>0.300537</td>\n",
       "      <td>0.395294</td>\n",
       "      <td>0.120690</td>\n",
       "      <td>0.062724</td>\n",
       "      <td>0.082547</td>\n",
       "      <td>0.268041</td>\n",
       "      <td>0.139535</td>\n",
       "      <td>0.183529</td>\n",
       "      <td>0.167214</td>\n",
       "      <td>0.873743</td>\n",
       "      <td>0.845592</td>\n",
       "      <td>0.859437</td>\n",
       "      <td>0.758264</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.375929</td>\n",
       "      <td>0.465930</td>\n",
       "      <td>0.416118</td>\n",
       "      <td>0.120536</td>\n",
       "      <td>0.149446</td>\n",
       "      <td>0.133443</td>\n",
       "      <td>0.182764</td>\n",
       "      <td>0.226519</td>\n",
       "      <td>0.202303</td>\n",
       "      <td>0.252446</td>\n",
       "      <td>0.859502</td>\n",
       "      <td>0.866692</td>\n",
       "      <td>0.863082</td>\n",
       "      <td>0.759479</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.593407</td>\n",
       "      <td>0.276215</td>\n",
       "      <td>0.376963</td>\n",
       "      <td>0.171271</td>\n",
       "      <td>0.079487</td>\n",
       "      <td>0.108581</td>\n",
       "      <td>0.313187</td>\n",
       "      <td>0.145780</td>\n",
       "      <td>0.198953</td>\n",
       "      <td>0.155327</td>\n",
       "      <td>0.871723</td>\n",
       "      <td>0.841290</td>\n",
       "      <td>0.856236</td>\n",
       "      <td>0.780752</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.474708</td>\n",
       "      <td>0.224265</td>\n",
       "      <td>0.304619</td>\n",
       "      <td>0.074219</td>\n",
       "      <td>0.034991</td>\n",
       "      <td>0.047559</td>\n",
       "      <td>0.233463</td>\n",
       "      <td>0.110294</td>\n",
       "      <td>0.149813</td>\n",
       "      <td>0.125147</td>\n",
       "      <td>0.848331</td>\n",
       "      <td>0.845004</td>\n",
       "      <td>0.846664</td>\n",
       "      <td>0.806250</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.641762</td>\n",
       "      <td>0.423515</td>\n",
       "      <td>0.510282</td>\n",
       "      <td>0.218810</td>\n",
       "      <td>0.144304</td>\n",
       "      <td>0.173913</td>\n",
       "      <td>0.262452</td>\n",
       "      <td>0.173198</td>\n",
       "      <td>0.208682</td>\n",
       "      <td>0.248638</td>\n",
       "      <td>0.891581</td>\n",
       "      <td>0.875034</td>\n",
       "      <td>0.883230</td>\n",
       "      <td>0.803890</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.538647</td>\n",
       "      <td>0.479570</td>\n",
       "      <td>0.507395</td>\n",
       "      <td>0.125908</td>\n",
       "      <td>0.112069</td>\n",
       "      <td>0.118586</td>\n",
       "      <td>0.227053</td>\n",
       "      <td>0.202151</td>\n",
       "      <td>0.213879</td>\n",
       "      <td>0.277183</td>\n",
       "      <td>0.866188</td>\n",
       "      <td>0.862894</td>\n",
       "      <td>0.864538</td>\n",
       "      <td>0.749808</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.643154</td>\n",
       "      <td>0.324948</td>\n",
       "      <td>0.431755</td>\n",
       "      <td>0.175000</td>\n",
       "      <td>0.088235</td>\n",
       "      <td>0.117318</td>\n",
       "      <td>0.327801</td>\n",
       "      <td>0.165618</td>\n",
       "      <td>0.220056</td>\n",
       "      <td>0.178629</td>\n",
       "      <td>0.877163</td>\n",
       "      <td>0.850221</td>\n",
       "      <td>0.863482</td>\n",
       "      <td>0.656146</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.578231</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.433673</td>\n",
       "      <td>0.174061</td>\n",
       "      <td>0.104294</td>\n",
       "      <td>0.130435</td>\n",
       "      <td>0.275510</td>\n",
       "      <td>0.165306</td>\n",
       "      <td>0.206633</td>\n",
       "      <td>0.192317</td>\n",
       "      <td>0.882935</td>\n",
       "      <td>0.866088</td>\n",
       "      <td>0.874430</td>\n",
       "      <td>0.769143</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.625532</td>\n",
       "      <td>0.360294</td>\n",
       "      <td>0.457232</td>\n",
       "      <td>0.179487</td>\n",
       "      <td>0.103194</td>\n",
       "      <td>0.131045</td>\n",
       "      <td>0.310638</td>\n",
       "      <td>0.178922</td>\n",
       "      <td>0.227061</td>\n",
       "      <td>0.218567</td>\n",
       "      <td>0.886147</td>\n",
       "      <td>0.874491</td>\n",
       "      <td>0.880280</td>\n",
       "      <td>0.717826</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.439024</td>\n",
       "      <td>0.040179</td>\n",
       "      <td>0.073620</td>\n",
       "      <td>0.025000</td>\n",
       "      <td>0.002237</td>\n",
       "      <td>0.004107</td>\n",
       "      <td>0.219512</td>\n",
       "      <td>0.020089</td>\n",
       "      <td>0.036810</td>\n",
       "      <td>0.024552</td>\n",
       "      <td>0.869980</td>\n",
       "      <td>0.787056</td>\n",
       "      <td>0.826443</td>\n",
       "      <td>0.902439</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.657143</td>\n",
       "      <td>0.123435</td>\n",
       "      <td>0.207831</td>\n",
       "      <td>0.221154</td>\n",
       "      <td>0.041219</td>\n",
       "      <td>0.069486</td>\n",
       "      <td>0.428571</td>\n",
       "      <td>0.080501</td>\n",
       "      <td>0.135542</td>\n",
       "      <td>0.076538</td>\n",
       "      <td>0.888603</td>\n",
       "      <td>0.822510</td>\n",
       "      <td>0.854280</td>\n",
       "      <td>0.778214</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.492135</td>\n",
       "      <td>0.403315</td>\n",
       "      <td>0.443320</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>0.132841</td>\n",
       "      <td>0.146045</td>\n",
       "      <td>0.249438</td>\n",
       "      <td>0.204420</td>\n",
       "      <td>0.224696</td>\n",
       "      <td>0.217505</td>\n",
       "      <td>0.870108</td>\n",
       "      <td>0.857141</td>\n",
       "      <td>0.863576</td>\n",
       "      <td>0.713283</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.670330</td>\n",
       "      <td>0.156010</td>\n",
       "      <td>0.253112</td>\n",
       "      <td>0.277778</td>\n",
       "      <td>0.064103</td>\n",
       "      <td>0.104167</td>\n",
       "      <td>0.406593</td>\n",
       "      <td>0.094629</td>\n",
       "      <td>0.153527</td>\n",
       "      <td>0.104713</td>\n",
       "      <td>0.888438</td>\n",
       "      <td>0.814185</td>\n",
       "      <td>0.849692</td>\n",
       "      <td>0.771429</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.576642</td>\n",
       "      <td>0.145221</td>\n",
       "      <td>0.232012</td>\n",
       "      <td>0.161765</td>\n",
       "      <td>0.040516</td>\n",
       "      <td>0.064801</td>\n",
       "      <td>0.394161</td>\n",
       "      <td>0.099265</td>\n",
       "      <td>0.158590</td>\n",
       "      <td>0.088274</td>\n",
       "      <td>0.877854</td>\n",
       "      <td>0.840195</td>\n",
       "      <td>0.858612</td>\n",
       "      <td>0.725000</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.573991</td>\n",
       "      <td>0.161820</td>\n",
       "      <td>0.252465</td>\n",
       "      <td>0.175676</td>\n",
       "      <td>0.049367</td>\n",
       "      <td>0.077075</td>\n",
       "      <td>0.313901</td>\n",
       "      <td>0.088496</td>\n",
       "      <td>0.138067</td>\n",
       "      <td>0.093237</td>\n",
       "      <td>0.884172</td>\n",
       "      <td>0.848460</td>\n",
       "      <td>0.865948</td>\n",
       "      <td>0.808276</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.596364</td>\n",
       "      <td>0.352688</td>\n",
       "      <td>0.443243</td>\n",
       "      <td>0.175182</td>\n",
       "      <td>0.103448</td>\n",
       "      <td>0.130081</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>0.189247</td>\n",
       "      <td>0.237838</td>\n",
       "      <td>0.231570</td>\n",
       "      <td>0.878931</td>\n",
       "      <td>0.862478</td>\n",
       "      <td>0.870627</td>\n",
       "      <td>0.733540</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.505076</td>\n",
       "      <td>0.417191</td>\n",
       "      <td>0.456946</td>\n",
       "      <td>0.124682</td>\n",
       "      <td>0.102941</td>\n",
       "      <td>0.112773</td>\n",
       "      <td>0.210660</td>\n",
       "      <td>0.174004</td>\n",
       "      <td>0.190586</td>\n",
       "      <td>0.256675</td>\n",
       "      <td>0.873886</td>\n",
       "      <td>0.863633</td>\n",
       "      <td>0.868729</td>\n",
       "      <td>0.745855</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.570136</td>\n",
       "      <td>0.257143</td>\n",
       "      <td>0.354430</td>\n",
       "      <td>0.163636</td>\n",
       "      <td>0.073620</td>\n",
       "      <td>0.101551</td>\n",
       "      <td>0.280543</td>\n",
       "      <td>0.126531</td>\n",
       "      <td>0.174402</td>\n",
       "      <td>0.157023</td>\n",
       "      <td>0.861873</td>\n",
       "      <td>0.851316</td>\n",
       "      <td>0.856562</td>\n",
       "      <td>0.773488</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.406863</td>\n",
       "      <td>0.688797</td>\n",
       "      <td>0.511556</td>\n",
       "      <td>0.142506</td>\n",
       "      <td>0.241667</td>\n",
       "      <td>0.179289</td>\n",
       "      <td>0.181373</td>\n",
       "      <td>0.307054</td>\n",
       "      <td>0.228043</td>\n",
       "      <td>0.376730</td>\n",
       "      <td>0.877556</td>\n",
       "      <td>0.892200</td>\n",
       "      <td>0.884817</td>\n",
       "      <td>0.753593</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.573661</td>\n",
       "      <td>0.683511</td>\n",
       "      <td>0.623786</td>\n",
       "      <td>0.286353</td>\n",
       "      <td>0.341333</td>\n",
       "      <td>0.311436</td>\n",
       "      <td>0.303571</td>\n",
       "      <td>0.361702</td>\n",
       "      <td>0.330097</td>\n",
       "      <td>0.424929</td>\n",
       "      <td>0.894980</td>\n",
       "      <td>0.910448</td>\n",
       "      <td>0.902648</td>\n",
       "      <td>0.751880</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.155635</td>\n",
       "      <td>0.696000</td>\n",
       "      <td>0.254386</td>\n",
       "      <td>0.062724</td>\n",
       "      <td>0.282258</td>\n",
       "      <td>0.102639</td>\n",
       "      <td>0.100179</td>\n",
       "      <td>0.448000</td>\n",
       "      <td>0.163743</td>\n",
       "      <td>0.278696</td>\n",
       "      <td>0.833895</td>\n",
       "      <td>0.902564</td>\n",
       "      <td>0.866872</td>\n",
       "      <td>0.782797</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.425414</td>\n",
       "      <td>0.598446</td>\n",
       "      <td>0.497309</td>\n",
       "      <td>0.129151</td>\n",
       "      <td>0.181818</td>\n",
       "      <td>0.151025</td>\n",
       "      <td>0.206262</td>\n",
       "      <td>0.290155</td>\n",
       "      <td>0.241119</td>\n",
       "      <td>0.315768</td>\n",
       "      <td>0.866292</td>\n",
       "      <td>0.879441</td>\n",
       "      <td>0.872817</td>\n",
       "      <td>0.698828</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.352941</td>\n",
       "      <td>0.641860</td>\n",
       "      <td>0.455446</td>\n",
       "      <td>0.138462</td>\n",
       "      <td>0.252336</td>\n",
       "      <td>0.178808</td>\n",
       "      <td>0.189258</td>\n",
       "      <td>0.344186</td>\n",
       "      <td>0.244224</td>\n",
       "      <td>0.330321</td>\n",
       "      <td>0.855917</td>\n",
       "      <td>0.881801</td>\n",
       "      <td>0.868666</td>\n",
       "      <td>0.746250</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.288603</td>\n",
       "      <td>0.716895</td>\n",
       "      <td>0.411533</td>\n",
       "      <td>0.117864</td>\n",
       "      <td>0.293578</td>\n",
       "      <td>0.168200</td>\n",
       "      <td>0.141544</td>\n",
       "      <td>0.351598</td>\n",
       "      <td>0.201835</td>\n",
       "      <td>0.350978</td>\n",
       "      <td>0.865709</td>\n",
       "      <td>0.888996</td>\n",
       "      <td>0.877198</td>\n",
       "      <td>0.663286</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.355247</td>\n",
       "      <td>0.776243</td>\n",
       "      <td>0.487424</td>\n",
       "      <td>0.121519</td>\n",
       "      <td>0.265928</td>\n",
       "      <td>0.166811</td>\n",
       "      <td>0.156764</td>\n",
       "      <td>0.342541</td>\n",
       "      <td>0.215091</td>\n",
       "      <td>0.380027</td>\n",
       "      <td>0.869647</td>\n",
       "      <td>0.898737</td>\n",
       "      <td>0.883953</td>\n",
       "      <td>0.746076</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.425806</td>\n",
       "      <td>0.664430</td>\n",
       "      <td>0.519004</td>\n",
       "      <td>0.159483</td>\n",
       "      <td>0.249158</td>\n",
       "      <td>0.194481</td>\n",
       "      <td>0.225806</td>\n",
       "      <td>0.352349</td>\n",
       "      <td>0.275229</td>\n",
       "      <td>0.374737</td>\n",
       "      <td>0.865592</td>\n",
       "      <td>0.885358</td>\n",
       "      <td>0.875364</td>\n",
       "      <td>0.690962</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.257862</td>\n",
       "      <td>0.661290</td>\n",
       "      <td>0.371041</td>\n",
       "      <td>0.077731</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.111952</td>\n",
       "      <td>0.142558</td>\n",
       "      <td>0.365591</td>\n",
       "      <td>0.205128</td>\n",
       "      <td>0.347559</td>\n",
       "      <td>0.848116</td>\n",
       "      <td>0.877834</td>\n",
       "      <td>0.862719</td>\n",
       "      <td>0.787711</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.259184</td>\n",
       "      <td>0.755952</td>\n",
       "      <td>0.386018</td>\n",
       "      <td>0.094070</td>\n",
       "      <td>0.275449</td>\n",
       "      <td>0.140244</td>\n",
       "      <td>0.136735</td>\n",
       "      <td>0.398810</td>\n",
       "      <td>0.203647</td>\n",
       "      <td>0.400316</td>\n",
       "      <td>0.854454</td>\n",
       "      <td>0.907349</td>\n",
       "      <td>0.880107</td>\n",
       "      <td>0.770839</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.313725</td>\n",
       "      <td>0.603774</td>\n",
       "      <td>0.412903</td>\n",
       "      <td>0.098280</td>\n",
       "      <td>0.189573</td>\n",
       "      <td>0.129450</td>\n",
       "      <td>0.151961</td>\n",
       "      <td>0.292453</td>\n",
       "      <td>0.200000</td>\n",
       "      <td>0.306128</td>\n",
       "      <td>0.871588</td>\n",
       "      <td>0.889547</td>\n",
       "      <td>0.880476</td>\n",
       "      <td>0.753593</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.274554</td>\n",
       "      <td>0.740964</td>\n",
       "      <td>0.400651</td>\n",
       "      <td>0.098434</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.143791</td>\n",
       "      <td>0.136161</td>\n",
       "      <td>0.367470</td>\n",
       "      <td>0.198697</td>\n",
       "      <td>0.380722</td>\n",
       "      <td>0.849507</td>\n",
       "      <td>0.892210</td>\n",
       "      <td>0.870335</td>\n",
       "      <td>0.751880</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.194991</td>\n",
       "      <td>0.633721</td>\n",
       "      <td>0.298222</td>\n",
       "      <td>0.060932</td>\n",
       "      <td>0.198830</td>\n",
       "      <td>0.093278</td>\n",
       "      <td>0.114490</td>\n",
       "      <td>0.372093</td>\n",
       "      <td>0.175103</td>\n",
       "      <td>0.277584</td>\n",
       "      <td>0.837409</td>\n",
       "      <td>0.890002</td>\n",
       "      <td>0.862905</td>\n",
       "      <td>0.782797</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.265193</td>\n",
       "      <td>0.566929</td>\n",
       "      <td>0.361355</td>\n",
       "      <td>0.101476</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.138365</td>\n",
       "      <td>0.147330</td>\n",
       "      <td>0.314961</td>\n",
       "      <td>0.200753</td>\n",
       "      <td>0.279112</td>\n",
       "      <td>0.852394</td>\n",
       "      <td>0.870397</td>\n",
       "      <td>0.861301</td>\n",
       "      <td>0.698828</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.291560</td>\n",
       "      <td>0.708075</td>\n",
       "      <td>0.413043</td>\n",
       "      <td>0.123077</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.174545</td>\n",
       "      <td>0.150895</td>\n",
       "      <td>0.366460</td>\n",
       "      <td>0.213768</td>\n",
       "      <td>0.360347</td>\n",
       "      <td>0.844637</td>\n",
       "      <td>0.895904</td>\n",
       "      <td>0.869516</td>\n",
       "      <td>0.746250</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.205882</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.314607</td>\n",
       "      <td>0.073665</td>\n",
       "      <td>0.239521</td>\n",
       "      <td>0.112676</td>\n",
       "      <td>0.102941</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.157303</td>\n",
       "      <td>0.307849</td>\n",
       "      <td>0.855661</td>\n",
       "      <td>0.887466</td>\n",
       "      <td>0.871273</td>\n",
       "      <td>0.663286</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.316056</td>\n",
       "      <td>0.771605</td>\n",
       "      <td>0.448430</td>\n",
       "      <td>0.110127</td>\n",
       "      <td>0.269350</td>\n",
       "      <td>0.156334</td>\n",
       "      <td>0.149178</td>\n",
       "      <td>0.364198</td>\n",
       "      <td>0.211659</td>\n",
       "      <td>0.379091</td>\n",
       "      <td>0.863157</td>\n",
       "      <td>0.900279</td>\n",
       "      <td>0.881327</td>\n",
       "      <td>0.746076</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.717593</td>\n",
       "      <td>0.455213</td>\n",
       "      <td>0.112069</td>\n",
       "      <td>0.241860</td>\n",
       "      <td>0.153166</td>\n",
       "      <td>0.180645</td>\n",
       "      <td>0.388889</td>\n",
       "      <td>0.246696</td>\n",
       "      <td>0.346264</td>\n",
       "      <td>0.852360</td>\n",
       "      <td>0.881079</td>\n",
       "      <td>0.866481</td>\n",
       "      <td>0.690962</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.291405</td>\n",
       "      <td>0.652582</td>\n",
       "      <td>0.402899</td>\n",
       "      <td>0.081933</td>\n",
       "      <td>0.183962</td>\n",
       "      <td>0.113372</td>\n",
       "      <td>0.138365</td>\n",
       "      <td>0.309859</td>\n",
       "      <td>0.191304</td>\n",
       "      <td>0.313241</td>\n",
       "      <td>0.848929</td>\n",
       "      <td>0.878730</td>\n",
       "      <td>0.863572</td>\n",
       "      <td>0.787711</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.255102</td>\n",
       "      <td>0.694444</td>\n",
       "      <td>0.373134</td>\n",
       "      <td>0.092025</td>\n",
       "      <td>0.251397</td>\n",
       "      <td>0.134731</td>\n",
       "      <td>0.153061</td>\n",
       "      <td>0.416667</td>\n",
       "      <td>0.223881</td>\n",
       "      <td>0.331269</td>\n",
       "      <td>0.851525</td>\n",
       "      <td>0.900912</td>\n",
       "      <td>0.875523</td>\n",
       "      <td>0.770839</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.693989</td>\n",
       "      <td>0.311275</td>\n",
       "      <td>0.429780</td>\n",
       "      <td>0.296703</td>\n",
       "      <td>0.132678</td>\n",
       "      <td>0.183362</td>\n",
       "      <td>0.344262</td>\n",
       "      <td>0.154412</td>\n",
       "      <td>0.213198</td>\n",
       "      <td>0.204417</td>\n",
       "      <td>0.905186</td>\n",
       "      <td>0.872219</td>\n",
       "      <td>0.888397</td>\n",
       "      <td>0.809924</td>\n",
       "      <td>GPT</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.720339</td>\n",
       "      <td>0.569196</td>\n",
       "      <td>0.635910</td>\n",
       "      <td>0.368272</td>\n",
       "      <td>0.290828</td>\n",
       "      <td>0.325000</td>\n",
       "      <td>0.389831</td>\n",
       "      <td>0.308036</td>\n",
       "      <td>0.344140</td>\n",
       "      <td>0.351699</td>\n",
       "      <td>0.910824</td>\n",
       "      <td>0.895340</td>\n",
       "      <td>0.903016</td>\n",
       "      <td>0.766820</td>\n",
       "      <td>GPT</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.701987</td>\n",
       "      <td>0.189624</td>\n",
       "      <td>0.298592</td>\n",
       "      <td>0.240000</td>\n",
       "      <td>0.064516</td>\n",
       "      <td>0.101695</td>\n",
       "      <td>0.377483</td>\n",
       "      <td>0.101968</td>\n",
       "      <td>0.160563</td>\n",
       "      <td>0.108066</td>\n",
       "      <td>0.900727</td>\n",
       "      <td>0.832237</td>\n",
       "      <td>0.865129</td>\n",
       "      <td>0.745294</td>\n",
       "      <td>GPT</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.574519</td>\n",
       "      <td>0.440147</td>\n",
       "      <td>0.498436</td>\n",
       "      <td>0.190361</td>\n",
       "      <td>0.145756</td>\n",
       "      <td>0.165099</td>\n",
       "      <td>0.271635</td>\n",
       "      <td>0.208103</td>\n",
       "      <td>0.235662</td>\n",
       "      <td>0.241270</td>\n",
       "      <td>0.878602</td>\n",
       "      <td>0.870083</td>\n",
       "      <td>0.874322</td>\n",
       "      <td>0.692970</td>\n",
       "      <td>GPT</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.681592</td>\n",
       "      <td>0.350384</td>\n",
       "      <td>0.462838</td>\n",
       "      <td>0.255000</td>\n",
       "      <td>0.130769</td>\n",
       "      <td>0.172881</td>\n",
       "      <td>0.348259</td>\n",
       "      <td>0.179028</td>\n",
       "      <td>0.236486</td>\n",
       "      <td>0.231868</td>\n",
       "      <td>0.888411</td>\n",
       "      <td>0.850751</td>\n",
       "      <td>0.869173</td>\n",
       "      <td>0.794474</td>\n",
       "      <td>GPT</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.831933</td>\n",
       "      <td>0.363971</td>\n",
       "      <td>0.506394</td>\n",
       "      <td>0.443038</td>\n",
       "      <td>0.193370</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.445378</td>\n",
       "      <td>0.194853</td>\n",
       "      <td>0.271100</td>\n",
       "      <td>0.204389</td>\n",
       "      <td>0.902546</td>\n",
       "      <td>0.880776</td>\n",
       "      <td>0.891528</td>\n",
       "      <td>0.673757</td>\n",
       "      <td>GPT</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.750617</td>\n",
       "      <td>0.384324</td>\n",
       "      <td>0.508361</td>\n",
       "      <td>0.254950</td>\n",
       "      <td>0.130380</td>\n",
       "      <td>0.172529</td>\n",
       "      <td>0.320988</td>\n",
       "      <td>0.164349</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.221136</td>\n",
       "      <td>0.899115</td>\n",
       "      <td>0.872424</td>\n",
       "      <td>0.885568</td>\n",
       "      <td>0.714719</td>\n",
       "      <td>GPT</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.736486</td>\n",
       "      <td>0.468817</td>\n",
       "      <td>0.572930</td>\n",
       "      <td>0.281356</td>\n",
       "      <td>0.178879</td>\n",
       "      <td>0.218709</td>\n",
       "      <td>0.368243</td>\n",
       "      <td>0.234409</td>\n",
       "      <td>0.286465</td>\n",
       "      <td>0.279998</td>\n",
       "      <td>0.891702</td>\n",
       "      <td>0.869115</td>\n",
       "      <td>0.880263</td>\n",
       "      <td>0.725020</td>\n",
       "      <td>GPT</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.656410</td>\n",
       "      <td>0.268344</td>\n",
       "      <td>0.380952</td>\n",
       "      <td>0.195876</td>\n",
       "      <td>0.079832</td>\n",
       "      <td>0.113433</td>\n",
       "      <td>0.338462</td>\n",
       "      <td>0.138365</td>\n",
       "      <td>0.196429</td>\n",
       "      <td>0.165094</td>\n",
       "      <td>0.881440</td>\n",
       "      <td>0.854395</td>\n",
       "      <td>0.867707</td>\n",
       "      <td>0.774795</td>\n",
       "      <td>GPT</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.713656</td>\n",
       "      <td>0.330612</td>\n",
       "      <td>0.451883</td>\n",
       "      <td>0.252212</td>\n",
       "      <td>0.116564</td>\n",
       "      <td>0.159441</td>\n",
       "      <td>0.361233</td>\n",
       "      <td>0.167347</td>\n",
       "      <td>0.228731</td>\n",
       "      <td>0.212329</td>\n",
       "      <td>0.889347</td>\n",
       "      <td>0.864533</td>\n",
       "      <td>0.876765</td>\n",
       "      <td>0.759101</td>\n",
       "      <td>GPT</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rouge1-P  rouge1-R  rouge1-F1  rouge2-P  rouge2-R  rouge2-F1  rougeL-P  \\\n",
       "0   0.674419  0.213235   0.324022  0.171875  0.054054   0.082243  0.372093   \n",
       "1   0.622222  0.062500   0.113590  0.204545  0.020134   0.036660  0.355556   \n",
       "2   0.646018  0.130590   0.217262  0.241071  0.048387   0.080597  0.442478   \n",
       "3   0.614035  0.064457   0.116667  0.232143  0.023985   0.043478  0.473684   \n",
       "4   0.743590  0.148338   0.247335  0.324675  0.064103   0.107066  0.435897   \n",
       "5   0.572650  0.123162   0.202723  0.206897  0.044199   0.072838  0.393162   \n",
       "6   0.612732  0.292035   0.395548  0.228723  0.108861   0.147513  0.275862   \n",
       "7   0.632479  0.318280   0.423462  0.171674  0.086207   0.114778  0.333333   \n",
       "8   0.433420  0.348008   0.386047  0.086387  0.069328   0.076923  0.190601   \n",
       "9   0.657143  0.187755   0.292063  0.237410  0.067485   0.105096  0.378571   \n",
       "10  0.646512  0.340686   0.446228  0.177570  0.093366   0.122383  0.339535   \n",
       "11  0.498947  0.529018   0.513543  0.141350  0.149888   0.145494  0.204211   \n",
       "12  0.577320  0.300537   0.395294  0.120690  0.062724   0.082547  0.268041   \n",
       "13  0.375929  0.465930   0.416118  0.120536  0.149446   0.133443  0.182764   \n",
       "14  0.593407  0.276215   0.376963  0.171271  0.079487   0.108581  0.313187   \n",
       "15  0.474708  0.224265   0.304619  0.074219  0.034991   0.047559  0.233463   \n",
       "16  0.641762  0.423515   0.510282  0.218810  0.144304   0.173913  0.262452   \n",
       "17  0.538647  0.479570   0.507395  0.125908  0.112069   0.118586  0.227053   \n",
       "18  0.643154  0.324948   0.431755  0.175000  0.088235   0.117318  0.327801   \n",
       "19  0.578231  0.346939   0.433673  0.174061  0.104294   0.130435  0.275510   \n",
       "20  0.625532  0.360294   0.457232  0.179487  0.103194   0.131045  0.310638   \n",
       "21  0.439024  0.040179   0.073620  0.025000  0.002237   0.004107  0.219512   \n",
       "22  0.657143  0.123435   0.207831  0.221154  0.041219   0.069486  0.428571   \n",
       "23  0.492135  0.403315   0.443320  0.162162  0.132841   0.146045  0.249438   \n",
       "24  0.670330  0.156010   0.253112  0.277778  0.064103   0.104167  0.406593   \n",
       "25  0.576642  0.145221   0.232012  0.161765  0.040516   0.064801  0.394161   \n",
       "26  0.573991  0.161820   0.252465  0.175676  0.049367   0.077075  0.313901   \n",
       "27  0.596364  0.352688   0.443243  0.175182  0.103448   0.130081  0.320000   \n",
       "28  0.505076  0.417191   0.456946  0.124682  0.102941   0.112773  0.210660   \n",
       "29  0.570136  0.257143   0.354430  0.163636  0.073620   0.101551  0.280543   \n",
       "30  0.406863  0.688797   0.511556  0.142506  0.241667   0.179289  0.181373   \n",
       "31  0.573661  0.683511   0.623786  0.286353  0.341333   0.311436  0.303571   \n",
       "32  0.155635  0.696000   0.254386  0.062724  0.282258   0.102639  0.100179   \n",
       "33  0.425414  0.598446   0.497309  0.129151  0.181818   0.151025  0.206262   \n",
       "34  0.352941  0.641860   0.455446  0.138462  0.252336   0.178808  0.189258   \n",
       "35  0.288603  0.716895   0.411533  0.117864  0.293578   0.168200  0.141544   \n",
       "36  0.355247  0.776243   0.487424  0.121519  0.265928   0.166811  0.156764   \n",
       "37  0.425806  0.664430   0.519004  0.159483  0.249158   0.194481  0.225806   \n",
       "38  0.257862  0.661290   0.371041  0.077731  0.200000   0.111952  0.142558   \n",
       "39  0.259184  0.755952   0.386018  0.094070  0.275449   0.140244  0.136735   \n",
       "40  0.313725  0.603774   0.412903  0.098280  0.189573   0.129450  0.151961   \n",
       "41  0.274554  0.740964   0.400651  0.098434  0.266667   0.143791  0.136161   \n",
       "42  0.194991  0.633721   0.298222  0.060932  0.198830   0.093278  0.114490   \n",
       "43  0.265193  0.566929   0.361355  0.101476  0.217391   0.138365  0.147330   \n",
       "44  0.291560  0.708075   0.413043  0.123077  0.300000   0.174545  0.150895   \n",
       "45  0.205882  0.666667   0.314607  0.073665  0.239521   0.112676  0.102941   \n",
       "46  0.316056  0.771605   0.448430  0.110127  0.269350   0.156334  0.149178   \n",
       "47  0.333333  0.717593   0.455213  0.112069  0.241860   0.153166  0.180645   \n",
       "48  0.291405  0.652582   0.402899  0.081933  0.183962   0.113372  0.138365   \n",
       "49  0.255102  0.694444   0.373134  0.092025  0.251397   0.134731  0.153061   \n",
       "50  0.693989  0.311275   0.429780  0.296703  0.132678   0.183362  0.344262   \n",
       "51  0.720339  0.569196   0.635910  0.368272  0.290828   0.325000  0.389831   \n",
       "52  0.701987  0.189624   0.298592  0.240000  0.064516   0.101695  0.377483   \n",
       "53  0.574519  0.440147   0.498436  0.190361  0.145756   0.165099  0.271635   \n",
       "54  0.681592  0.350384   0.462838  0.255000  0.130769   0.172881  0.348259   \n",
       "55  0.831933  0.363971   0.506394  0.443038  0.193370   0.269231  0.445378   \n",
       "56  0.750617  0.384324   0.508361  0.254950  0.130380   0.172529  0.320988   \n",
       "57  0.736486  0.468817   0.572930  0.281356  0.178879   0.218709  0.368243   \n",
       "58  0.656410  0.268344   0.380952  0.195876  0.079832   0.113433  0.338462   \n",
       "59  0.713656  0.330612   0.451883  0.252212  0.116564   0.159441  0.361233   \n",
       "\n",
       "    rougeL-R  rougeL-F1    METEOR  BERTScore Precision  BERTScore Recall  \\\n",
       "0   0.117647   0.178771  0.135188             0.881479          0.852133   \n",
       "1   0.035714   0.064909  0.035565             0.882748          0.793773   \n",
       "2   0.089445   0.148810  0.081500             0.877768          0.818323   \n",
       "3   0.049724   0.090000  0.042399             0.883883          0.800092   \n",
       "4   0.086957   0.144989  0.102564             0.894963          0.814272   \n",
       "5   0.084559   0.139183  0.081424             0.886397          0.837483   \n",
       "6   0.131479   0.178082  0.185130             0.882320          0.866433   \n",
       "7   0.167742   0.223176  0.191558             0.875856          0.852887   \n",
       "8   0.153040   0.169767  0.198817             0.865793          0.856556   \n",
       "9   0.108163   0.168254  0.110851             0.878814          0.840818   \n",
       "10  0.178922   0.234350  0.204217             0.884044          0.874819   \n",
       "11  0.216518   0.210184  0.298219             0.871369          0.872314   \n",
       "12  0.139535   0.183529  0.167214             0.873743          0.845592   \n",
       "13  0.226519   0.202303  0.252446             0.859502          0.866692   \n",
       "14  0.145780   0.198953  0.155327             0.871723          0.841290   \n",
       "15  0.110294   0.149813  0.125147             0.848331          0.845004   \n",
       "16  0.173198   0.208682  0.248638             0.891581          0.875034   \n",
       "17  0.202151   0.213879  0.277183             0.866188          0.862894   \n",
       "18  0.165618   0.220056  0.178629             0.877163          0.850221   \n",
       "19  0.165306   0.206633  0.192317             0.882935          0.866088   \n",
       "20  0.178922   0.227061  0.218567             0.886147          0.874491   \n",
       "21  0.020089   0.036810  0.024552             0.869980          0.787056   \n",
       "22  0.080501   0.135542  0.076538             0.888603          0.822510   \n",
       "23  0.204420   0.224696  0.217505             0.870108          0.857141   \n",
       "24  0.094629   0.153527  0.104713             0.888438          0.814185   \n",
       "25  0.099265   0.158590  0.088274             0.877854          0.840195   \n",
       "26  0.088496   0.138067  0.093237             0.884172          0.848460   \n",
       "27  0.189247   0.237838  0.231570             0.878931          0.862478   \n",
       "28  0.174004   0.190586  0.256675             0.873886          0.863633   \n",
       "29  0.126531   0.174402  0.157023             0.861873          0.851316   \n",
       "30  0.307054   0.228043  0.376730             0.877556          0.892200   \n",
       "31  0.361702   0.330097  0.424929             0.894980          0.910448   \n",
       "32  0.448000   0.163743  0.278696             0.833895          0.902564   \n",
       "33  0.290155   0.241119  0.315768             0.866292          0.879441   \n",
       "34  0.344186   0.244224  0.330321             0.855917          0.881801   \n",
       "35  0.351598   0.201835  0.350978             0.865709          0.888996   \n",
       "36  0.342541   0.215091  0.380027             0.869647          0.898737   \n",
       "37  0.352349   0.275229  0.374737             0.865592          0.885358   \n",
       "38  0.365591   0.205128  0.347559             0.848116          0.877834   \n",
       "39  0.398810   0.203647  0.400316             0.854454          0.907349   \n",
       "40  0.292453   0.200000  0.306128             0.871588          0.889547   \n",
       "41  0.367470   0.198697  0.380722             0.849507          0.892210   \n",
       "42  0.372093   0.175103  0.277584             0.837409          0.890002   \n",
       "43  0.314961   0.200753  0.279112             0.852394          0.870397   \n",
       "44  0.366460   0.213768  0.360347             0.844637          0.895904   \n",
       "45  0.333333   0.157303  0.307849             0.855661          0.887466   \n",
       "46  0.364198   0.211659  0.379091             0.863157          0.900279   \n",
       "47  0.388889   0.246696  0.346264             0.852360          0.881079   \n",
       "48  0.309859   0.191304  0.313241             0.848929          0.878730   \n",
       "49  0.416667   0.223881  0.331269             0.851525          0.900912   \n",
       "50  0.154412   0.213198  0.204417             0.905186          0.872219   \n",
       "51  0.308036   0.344140  0.351699             0.910824          0.895340   \n",
       "52  0.101968   0.160563  0.108066             0.900727          0.832237   \n",
       "53  0.208103   0.235662  0.241270             0.878602          0.870083   \n",
       "54  0.179028   0.236486  0.231868             0.888411          0.850751   \n",
       "55  0.194853   0.271100  0.204389             0.902546          0.880776   \n",
       "56  0.164349   0.217391  0.221136             0.899115          0.872424   \n",
       "57  0.234409   0.286465  0.279998             0.891702          0.869115   \n",
       "58  0.138365   0.196429  0.165094             0.881440          0.854395   \n",
       "59  0.167347   0.228731  0.212329             0.889347          0.864533   \n",
       "\n",
       "    BERTScore F1     MATTR  model           tech  \n",
       "0       0.866558  0.784615  Gemma           Base  \n",
       "1       0.835900  0.800000  Gemma           Base  \n",
       "2       0.847004  0.758125  Gemma           Base  \n",
       "3       0.839903  0.780000  Gemma           Base  \n",
       "4       0.852713  0.713103  Gemma           Base  \n",
       "5       0.861246  0.755882  Gemma           Base  \n",
       "6       0.874305  0.778293  Gemma           Base  \n",
       "7       0.864219  0.755459  Gemma           Base  \n",
       "8       0.861150  0.669701  Gemma           Base  \n",
       "9       0.859396  0.787033  Gemma           Base  \n",
       "10      0.879408  0.741098  Gemma  Decomposition  \n",
       "11      0.871841  0.754178  Gemma  Decomposition  \n",
       "12      0.859437  0.758264  Gemma  Decomposition  \n",
       "13      0.863082  0.759479  Gemma  Decomposition  \n",
       "14      0.856236  0.780752  Gemma  Decomposition  \n",
       "15      0.846664  0.806250  Gemma  Decomposition  \n",
       "16      0.883230  0.803890  Gemma  Decomposition  \n",
       "17      0.864538  0.749808  Gemma  Decomposition  \n",
       "18      0.863482  0.656146  Gemma  Decomposition  \n",
       "19      0.874430  0.769143  Gemma  Decomposition  \n",
       "20      0.880280  0.717826  Gemma           HyDE  \n",
       "21      0.826443  0.902439  Gemma           HyDE  \n",
       "22      0.854280  0.778214  Gemma           HyDE  \n",
       "23      0.863576  0.713283  Gemma           HyDE  \n",
       "24      0.849692  0.771429  Gemma           HyDE  \n",
       "25      0.858612  0.725000  Gemma           HyDE  \n",
       "26      0.865948  0.808276  Gemma           HyDE  \n",
       "27      0.870627  0.733540  Gemma           HyDE  \n",
       "28      0.868729  0.745855  Gemma           HyDE  \n",
       "29      0.856562  0.773488  Gemma           HyDE  \n",
       "30      0.884817  0.753593    GPT           Base  \n",
       "31      0.902648  0.751880    GPT           Base  \n",
       "32      0.866872  0.782797    GPT           Base  \n",
       "33      0.872817  0.698828    GPT           Base  \n",
       "34      0.868666  0.746250    GPT           Base  \n",
       "35      0.877198  0.663286    GPT           Base  \n",
       "36      0.883953  0.746076    GPT           Base  \n",
       "37      0.875364  0.690962    GPT           Base  \n",
       "38      0.862719  0.787711    GPT           Base  \n",
       "39      0.880107  0.770839    GPT           Base  \n",
       "40      0.880476  0.753593    GPT  Decomposition  \n",
       "41      0.870335  0.751880    GPT  Decomposition  \n",
       "42      0.862905  0.782797    GPT  Decomposition  \n",
       "43      0.861301  0.698828    GPT  Decomposition  \n",
       "44      0.869516  0.746250    GPT  Decomposition  \n",
       "45      0.871273  0.663286    GPT  Decomposition  \n",
       "46      0.881327  0.746076    GPT  Decomposition  \n",
       "47      0.866481  0.690962    GPT  Decomposition  \n",
       "48      0.863572  0.787711    GPT  Decomposition  \n",
       "49      0.875523  0.770839    GPT  Decomposition  \n",
       "50      0.888397  0.809924    GPT           HyDE  \n",
       "51      0.903016  0.766820    GPT           HyDE  \n",
       "52      0.865129  0.745294    GPT           HyDE  \n",
       "53      0.874322  0.692970    GPT           HyDE  \n",
       "54      0.869173  0.794474    GPT           HyDE  \n",
       "55      0.891528  0.673757    GPT           HyDE  \n",
       "56      0.885568  0.714719    GPT           HyDE  \n",
       "57      0.880263  0.725020    GPT           HyDE  \n",
       "58      0.867707  0.774795    GPT           HyDE  \n",
       "59      0.876765  0.759101    GPT           HyDE  "
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gemma  = pd.concat([\n",
    "    df_base.assign(model='Gemma', tech='Base'),\n",
    "    df_decomposition.assign(model='Gemma', tech='Decomposition'),\n",
    "    df_hyde.assign(model='Gemma', tech='HyDE')\n",
    "])\n",
    "\n",
    "df_gpt = pd.concat([\n",
    "    df_gpt_base.assign(model='GPT', tech='Base'),\n",
    "    df_gpt_decomposition.assign(model='GPT', tech='Decomposition'),\n",
    "    df_gpt_hyde.assign(model='GPT', tech='HyDE')\n",
    "])\n",
    "\n",
    "df_all = pd.concat([df_gemma, df_gpt], ignore_index=True)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>rouge1-P</th>\n",
       "      <th>rouge1-R</th>\n",
       "      <th>rouge1-F1</th>\n",
       "      <th>rouge2-P</th>\n",
       "      <th>rouge2-R</th>\n",
       "      <th>rouge2-F1</th>\n",
       "      <th>rougeL-P</th>\n",
       "      <th>rougeL-R</th>\n",
       "      <th>rougeL-F1</th>\n",
       "      <th>METEOR</th>\n",
       "      <th>BERTScore Precision</th>\n",
       "      <th>BERTScore Recall</th>\n",
       "      <th>BERTScore F1</th>\n",
       "      <th>MATTR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>tech</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GPT</th>\n",
       "      <th>Base</th>\n",
       "      <td>0.3501</td>\n",
       "      <td>0.6883</td>\n",
       "      <td>0.4518</td>\n",
       "      <td>0.1330</td>\n",
       "      <td>0.2584</td>\n",
       "      <td>0.1705</td>\n",
       "      <td>0.1784</td>\n",
       "      <td>0.3562</td>\n",
       "      <td>0.2308</td>\n",
       "      <td>0.3580</td>\n",
       "      <td>0.8632</td>\n",
       "      <td>0.8925</td>\n",
       "      <td>0.8775</td>\n",
       "      <td>0.7392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decomposition</th>\n",
       "      <td>0.2742</td>\n",
       "      <td>0.6756</td>\n",
       "      <td>0.3880</td>\n",
       "      <td>0.0952</td>\n",
       "      <td>0.2359</td>\n",
       "      <td>0.1350</td>\n",
       "      <td>0.1425</td>\n",
       "      <td>0.3526</td>\n",
       "      <td>0.2019</td>\n",
       "      <td>0.3282</td>\n",
       "      <td>0.8527</td>\n",
       "      <td>0.8887</td>\n",
       "      <td>0.8703</td>\n",
       "      <td>0.7392</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HyDE</th>\n",
       "      <td>0.7062</td>\n",
       "      <td>0.3677</td>\n",
       "      <td>0.4746</td>\n",
       "      <td>0.2778</td>\n",
       "      <td>0.1464</td>\n",
       "      <td>0.1881</td>\n",
       "      <td>0.3566</td>\n",
       "      <td>0.1851</td>\n",
       "      <td>0.2390</td>\n",
       "      <td>0.2220</td>\n",
       "      <td>0.8948</td>\n",
       "      <td>0.8662</td>\n",
       "      <td>0.8802</td>\n",
       "      <td>0.7457</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Gemma</th>\n",
       "      <th>Base</th>\n",
       "      <td>0.6209</td>\n",
       "      <td>0.1888</td>\n",
       "      <td>0.2719</td>\n",
       "      <td>0.2105</td>\n",
       "      <td>0.0587</td>\n",
       "      <td>0.0867</td>\n",
       "      <td>0.3651</td>\n",
       "      <td>0.1024</td>\n",
       "      <td>0.1506</td>\n",
       "      <td>0.1165</td>\n",
       "      <td>0.8810</td>\n",
       "      <td>0.8333</td>\n",
       "      <td>0.8562</td>\n",
       "      <td>0.7582</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decomposition</th>\n",
       "      <td>0.5569</td>\n",
       "      <td>0.3712</td>\n",
       "      <td>0.4336</td>\n",
       "      <td>0.1499</td>\n",
       "      <td>0.1019</td>\n",
       "      <td>0.1180</td>\n",
       "      <td>0.2634</td>\n",
       "      <td>0.1724</td>\n",
       "      <td>0.2028</td>\n",
       "      <td>0.2099</td>\n",
       "      <td>0.8727</td>\n",
       "      <td>0.8600</td>\n",
       "      <td>0.8662</td>\n",
       "      <td>0.7579</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HyDE</th>\n",
       "      <td>0.5706</td>\n",
       "      <td>0.2417</td>\n",
       "      <td>0.3174</td>\n",
       "      <td>0.1667</td>\n",
       "      <td>0.0713</td>\n",
       "      <td>0.0941</td>\n",
       "      <td>0.3134</td>\n",
       "      <td>0.1256</td>\n",
       "      <td>0.1677</td>\n",
       "      <td>0.1469</td>\n",
       "      <td>0.8780</td>\n",
       "      <td>0.8421</td>\n",
       "      <td>0.8595</td>\n",
       "      <td>0.7669</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     rouge1-P  rouge1-R  rouge1-F1  rouge2-P  rouge2-R  \\\n",
       "model tech                                                               \n",
       "GPT   Base             0.3501    0.6883     0.4518    0.1330    0.2584   \n",
       "      Decomposition    0.2742    0.6756     0.3880    0.0952    0.2359   \n",
       "      HyDE             0.7062    0.3677     0.4746    0.2778    0.1464   \n",
       "Gemma Base             0.6209    0.1888     0.2719    0.2105    0.0587   \n",
       "      Decomposition    0.5569    0.3712     0.4336    0.1499    0.1019   \n",
       "      HyDE             0.5706    0.2417     0.3174    0.1667    0.0713   \n",
       "\n",
       "                     rouge2-F1  rougeL-P  rougeL-R  rougeL-F1  METEOR  \\\n",
       "model tech                                                              \n",
       "GPT   Base              0.1705    0.1784    0.3562     0.2308  0.3580   \n",
       "      Decomposition     0.1350    0.1425    0.3526     0.2019  0.3282   \n",
       "      HyDE              0.1881    0.3566    0.1851     0.2390  0.2220   \n",
       "Gemma Base              0.0867    0.3651    0.1024     0.1506  0.1165   \n",
       "      Decomposition     0.1180    0.2634    0.1724     0.2028  0.2099   \n",
       "      HyDE              0.0941    0.3134    0.1256     0.1677  0.1469   \n",
       "\n",
       "                     BERTScore Precision  BERTScore Recall  BERTScore F1  \\\n",
       "model tech                                                                 \n",
       "GPT   Base                        0.8632            0.8925        0.8775   \n",
       "      Decomposition               0.8527            0.8887        0.8703   \n",
       "      HyDE                        0.8948            0.8662        0.8802   \n",
       "Gemma Base                        0.8810            0.8333        0.8562   \n",
       "      Decomposition               0.8727            0.8600        0.8662   \n",
       "      HyDE                        0.8780            0.8421        0.8595   \n",
       "\n",
       "                      MATTR  \n",
       "model tech                   \n",
       "GPT   Base           0.7392  \n",
       "      Decomposition  0.7392  \n",
       "      HyDE           0.7457  \n",
       "Gemma Base           0.7582  \n",
       "      Decomposition  0.7579  \n",
       "      HyDE           0.7669  "
      ]
     },
     "execution_count": 75,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cols = df_all.select_dtypes('number').columns\n",
    "\n",
    "# Calculate the mean for each model and tech\n",
    "mean_table = (\n",
    "    df_all\n",
    "      .groupby(['model', 'tech'])[num_cols]\n",
    "      .mean()\n",
    "      .round(4)\n",
    ")\n",
    "mean_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [],
   "source": [
    "def find_strategy_winners_verbose(df, model_name):\n",
    "    \"\"\"\n",
    "    Identifies and prints:\n",
    "     - average of each metric by technique,\n",
    "     - winner per metric,\n",
    "     - total win counts,\n",
    "    for a given model.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Complete dataset.\n",
    "        model_name (str): Model to filter (e.g., \"Gemma\" or \"GPT\").\n",
    "    \"\"\"\n",
    "    # 1) Filter only the specified model\n",
    "    model_df = df[df[\"model\"] == model_name].copy()\n",
    "    \n",
    "    # 2) Choose numeric metrics\n",
    "    metric_cols = [\n",
    "        c for c in model_df.select_dtypes(\"number\").columns\n",
    "        if model_df[c].notna().any()\n",
    "    ]\n",
    "    \n",
    "    # 3) Average by technique\n",
    "    mean_table = model_df.groupby(\"tech\")[metric_cols].mean()\n",
    "    winner_per_metric = mean_table.idxmax()\n",
    "    win_counts = winner_per_metric.value_counts()\n",
    "    \n",
    "    # 4) Print outputs\n",
    "    print(f\"\\n=== AVERAGE OF EACH METRIC BY TECHNIQUE ({model_name}) ===\")\n",
    "    print(mean_table.T.round(4))\n",
    "    \n",
    "    print(f\"\\n=== TECHNIQUE WINNER PER METRIC ({model_name}) ===\")\n",
    "    print(winner_per_metric)\n",
    "    \n",
    "    print(f\"\\n=== WIN COUNTS ({model_name}) ===\")\n",
    "    print(win_counts)\n",
    "    \n",
    "    return mean_table, winner_per_metric, win_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ttest_cohen_d_between_techniques(df, model_name):\n",
    "    \"\"\"\n",
    "    Performs paired t-tests and calculates Cohen's d between all pairs\n",
    "    of techniques for a specified model.\n",
    "    \n",
    "    Args:\n",
    "        df (DataFrame): Complete dataset.\n",
    "        model_name (str): Model to filter (e.g., \"Gemma\" or \"GPT\").\n",
    "    \n",
    "    Returns:\n",
    "        stats_df (DataFrame): DataFrame containing metric, tech_A, tech_B, p_value, and cohen_d.\n",
    "    \"\"\"\n",
    "    from itertools import combinations\n",
    "    from scipy import stats\n",
    "\n",
    "    def cohen_d_paired(x, y):\n",
    "        diff = x - y\n",
    "        return diff.mean() / diff.std(ddof=1)\n",
    "    \n",
    "    # Filter model\n",
    "    model_df = df[df[\"model\"] == model_name].copy()\n",
    "    \n",
    "    # Identify numeric metrics\n",
    "    metric_cols = [\n",
    "        c for c in model_df.select_dtypes(\"number\").columns\n",
    "        if model_df[c].notna().any()\n",
    "    ]\n",
    "    \n",
    "    # Compare techniques pairwise\n",
    "    rows = []\n",
    "    for tech_a, tech_b in combinations(model_df[\"tech\"].unique(), 2):\n",
    "        a = model_df[model_df.tech == tech_a]\n",
    "        b = model_df[model_df.tech == tech_b]\n",
    "        for m in metric_cols:\n",
    "            x = a[m].dropna().reset_index(drop=True)\n",
    "            y = b[m].dropna().reset_index(drop=True)\n",
    "            x, y = x.align(y, join=\"inner\")  # Pair by original index\n",
    "            if len(x) < 2:\n",
    "                continue\n",
    "            t, p = stats.ttest_rel(x, y)\n",
    "            d = cohen_d_paired(x, y)\n",
    "            rows.append({\n",
    "                \"metric\": m,\n",
    "                \"tech_A\": tech_a,\n",
    "                \"tech_B\": tech_b,\n",
    "                \"p_value\": round(p, 4),\n",
    "                \"cohen_d\": round(d, 2)\n",
    "            })\n",
    "    \n",
    "    stats_df = pd.DataFrame(rows)\n",
    "    \n",
    "    print(f\"\\n=== t-test + Cohen's d Between Techniques ({model_name}) ===\")\n",
    "    print(stats_df.round(4))\n",
    "    \n",
    "    return stats_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_wins_from_stats(stats_df, model_name):\n",
    "    \"\"\"\n",
    "    Filters statistically significant results and identifies winners per technique.\n",
    "    \n",
    "    Args:\n",
    "        stats_df (DataFrame): DataFrame containing metric, tech_A, tech_B, p_value, and cohen_d.\n",
    "        model_name (str): Model name to use in printed outputs (e.g., \"Gemma\" or \"GPT\").\n",
    "    \n",
    "    Returns:\n",
    "        sig (DataFrame): Significant comparisons with winner assigned.\n",
    "        win_counts (Series): Number of wins per technique.\n",
    "    \"\"\"\n",
    "    import numpy as np\n",
    "    \n",
    "    # 1) Filter by p < 0.05 and |d| >= 0.5\n",
    "    sig = stats_df[(stats_df[\"p_value\"] < 0.05) & (stats_df[\"cohen_d\"].abs() >= 0.5)].copy()\n",
    "    \n",
    "    # 2) Decide the winner based on the sign of Cohen's d\n",
    "    sig[\"winner\"] = np.where(sig[\"cohen_d\"] > 0, sig[\"tech_A\"], sig[\"tech_B\"])\n",
    "    \n",
    "    # 3) Count wins\n",
    "    win_counts = sig[\"winner\"].value_counts()\n",
    "    \n",
    "    # 4) Print outputs\n",
    "    print(f\"\\n### Winner per technique ({model_name}; p<0.05 & |d|>=0.5) ###\")\n",
    "    print(win_counts)\n",
    "    \n",
    "    # 5) List metrics won by each technique\n",
    "    for tech in win_counts.index:\n",
    "        metrics = sig.loc[sig[\"winner\"] == tech, \"metric\"].tolist()\n",
    "        print(f\"\\n{tech} won: {', '.join(metrics)}\")\n",
    "    \n",
    "    return sig, win_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "def analyze_semantic_metrics(gemma, gpt, stat_gemma, stat_gpt, semantic_metrics, top_k=3, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Performs semantic metric analysis, printing:\n",
    "      1) Top-k techniques per metric by mean score with confidence intervals.\n",
    "      2) Statistical wins per technique (p<0.05 and |d|>=0.5).\n",
    "    \n",
    "    Args:\n",
    "        gemma (DataFrame): Subset of the dataset filtered for the Gemma model.\n",
    "        gpt (DataFrame): Subset of the dataset filtered for the GPT model.\n",
    "        stat_gemma (DataFrame): Statistical test results (paired t-test and Cohen's d) for Gemma.\n",
    "        stat_gpt (DataFrame): Statistical test results for GPT.\n",
    "        semantic_metrics (list): List of semantic metric names to analyze.\n",
    "        top_k (int, optional): Number of top techniques to display. Default is 3.\n",
    "        confidence_level (float, optional): Confidence level for intervals (e.g., 0.95 for 95%). Default is 0.95.\n",
    "    \n",
    "    Returns:\n",
    "        None\n",
    "    \"\"\"\n",
    "    z = 1.96  # z-score for 95% confidence interval\n",
    "    \n",
    "    print(f\"\\n=== Semantic Metric Analysis – Top {top_k} Techniques (Confidence Level: {int(confidence_level * 100)}%) ===\")\n",
    "    for m in semantic_metrics:\n",
    "        print(f\"\\n{m}:\")\n",
    "        \n",
    "        # GEMMA\n",
    "        grp_g = gemma.groupby('tech')[m]\n",
    "        desc_g = grp_g.mean().sort_values(ascending=False)\n",
    "        print(\"  GEMMA:\")\n",
    "        for rank, tech in enumerate(desc_g.index[:top_k], start=1):\n",
    "            mu = desc_g.loc[tech]\n",
    "            std = grp_g.std()[tech]\n",
    "            n = grp_g.count()[tech]\n",
    "            sem = std / math.sqrt(n)\n",
    "            ic = z * sem\n",
    "            print(f\"    {rank}º: {tech}: {mu:.3f} [95% CI: {mu-ic:.3f} – {mu+ic:.3f}] (n={n})\")\n",
    "        \n",
    "        # GPT\n",
    "        grp_p = gpt.groupby('tech')[m]\n",
    "        desc_p = grp_p.mean().sort_values(ascending=False)\n",
    "        print(\"  GPT:\")\n",
    "        for rank, tech in enumerate(desc_p.index[:top_k], start=1):\n",
    "            mu = desc_p.loc[tech]\n",
    "            std = grp_p.std()[tech]\n",
    "            n = grp_p.count()[tech]\n",
    "            sem = std / math.sqrt(n)\n",
    "            ic = z * sem\n",
    "            print(f\"    {rank}º: {tech}: {mu:.3f} [95% CI: {mu-ic:.3f} – {mu+ic:.3f}] (n={n})\")\n",
    "    \n",
    "    print(\"\\n=== Statistical Wins by Metric (p<0.05 and |d|>=0.5) ===\")\n",
    "    for m in semantic_metrics:\n",
    "        cnt_g = stat_gemma[stat_gemma.metric == m]['winner'].value_counts()\n",
    "        cnt_p = stat_gpt[stat_gpt.metric == m]['winner'].value_counts()\n",
    "        print(f\"\\n{m}:\")\n",
    "        print(f\"  Gemma: {cnt_g.to_dict() or 'no statistically significant wins'}\")\n",
    "        print(f\"  GPT  : {cnt_p.to_dict() or 'no statistically significant wins'}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [],
   "source": [
    "def summary(df_all, semantic_metrics=None, top_k=3, confidence_level=0.95):\n",
    "    \"\"\"\n",
    "    Runs a complete performance analysis across techniques for Gemma and GPT models.\n",
    "    \n",
    "    Steps:\n",
    "    1) Victories by average (mean).\n",
    "    2) Victories by statistical significance (paired t-test, p<0.05 and |d|>=0.5).\n",
    "    3) Semantic metrics detailed analysis: top-k by mean, statistical wins.\n",
    "    4) Summarizes all results into a structured table (grouped by Model first, then Criterion).\n",
    "    \n",
    "    Args:\n",
    "        df_all (DataFrame): Complete dataset containing all models, techniques, and metrics.\n",
    "        semantic_metrics (list, optional): List of semantic metric names to analyze. Defaults to common metrics.\n",
    "        top_k (int, optional): Number of top techniques to display per metric. Default is 3.\n",
    "        confidence_level (float, optional): Confidence level for intervals. Default is 0.95 (95%).\n",
    "    \n",
    "    Returns:\n",
    "        summary_table (DataFrame): Final structured table.\n",
    "    \"\"\"\n",
    "    import math\n",
    "    import numpy as np\n",
    "    import pandas as pd\n",
    "    from itertools import combinations\n",
    "    from scipy import stats\n",
    "    \n",
    "    # 1) Filter Gemma and GPT\n",
    "    gemma = df_all[df_all['model'] == 'Gemma']\n",
    "    gpt   = df_all[df_all['model'] == 'GPT']\n",
    "\n",
    "    # 2) Identify numeric metric columns\n",
    "    metric_cols = [\n",
    "        c for c in df_all.select_dtypes(include=np.number).columns\n",
    "        if df_all[c].notna().any()\n",
    "    ]\n",
    "    \n",
    "    # ————————————————————————————————\n",
    "    # Block 1: Wins by Average\n",
    "    # ————————————————————————————————\n",
    "    mean_gemma = gemma.groupby('tech')[metric_cols].mean()\n",
    "    mean_gpt   = gpt.groupby('tech')[metric_cols].mean()\n",
    "\n",
    "    win_mean_gemma = mean_gemma.idxmax().value_counts()\n",
    "    win_mean_gpt   = mean_gpt.idxmax().value_counts()\n",
    "\n",
    "    print(\"1) Which strategy wins within Gemma (by average)?\")\n",
    "    print(win_mean_gemma)\n",
    "\n",
    "    print(\"\\n2) Which strategy wins within GPT (by average)?\")\n",
    "    print(win_mean_gpt)\n",
    "\n",
    "    # ————————————————————————————————\n",
    "    # Block 2: Wins by Statistical Test\n",
    "    # ————————————————————————————————\n",
    "    def cohen_d_paired(x, y):\n",
    "        diff = x - y\n",
    "        return diff.mean() / diff.std(ddof=1)\n",
    "\n",
    "    def head_to_head(df_model):\n",
    "        rows = []\n",
    "        for tech_a, tech_b in combinations(df_model['tech'].unique(), 2):\n",
    "            a = df_model[df_model.tech == tech_a]\n",
    "            b = df_model[df_model.tech == tech_b]\n",
    "            for m in metric_cols:\n",
    "                xa = a[m].dropna().reset_index(drop=True)\n",
    "                xb = b[m].dropna().reset_index(drop=True)\n",
    "                xa, xb = xa.align(xb, join='inner')\n",
    "                if len(xa) < 2:\n",
    "                    continue\n",
    "                t, p = stats.ttest_rel(xa, xb)\n",
    "                d = cohen_d_paired(xa, xb)\n",
    "                if (p < 0.05) and (abs(d) >= 0.5):\n",
    "                    winner = tech_a if d > 0 else tech_b\n",
    "                    rows.append({'metric': m, 'winner': winner})\n",
    "        return pd.DataFrame(rows)\n",
    "\n",
    "    stat_gemma = head_to_head(gemma)\n",
    "    stat_gpt   = head_to_head(gpt)\n",
    "\n",
    "    win_stat_gemma = stat_gemma['winner'].value_counts()\n",
    "    win_stat_gpt   = stat_gpt['winner'].value_counts()\n",
    "\n",
    "    print(\"\\n3) Which strategy wins statistically within Gemma (p<0.05 & |d|>=0.5)?\")\n",
    "    print(win_stat_gemma)\n",
    "\n",
    "    print(\"\\n4) Which strategy wins statistically within GPT (p<0.05 & |d|>=0.5)?\")\n",
    "    print(win_stat_gpt)\n",
    "\n",
    "    # ————————————————————————————————\n",
    "    # Block 3: Build the final structured table\n",
    "    # ————————————————————————————————\n",
    "    summary_table = pd.DataFrame([\n",
    "        {'Model': 'Gemma', 'Criterion': 'By Average',          'Base': win_mean_gemma.get('Base', 0), 'HyDE': win_mean_gemma.get('HyDE', 0), 'Decomposition': win_mean_gemma.get('Decomposition', 0)},\n",
    "        {'Model': 'Gemma', 'Criterion': 'By Statistical Test', 'Base': win_stat_gemma.get('Base', 0), 'HyDE': win_stat_gemma.get('HyDE', 0), 'Decomposition': win_stat_gemma.get('Decomposition', 0)},\n",
    "        {'Model': 'GPT',   'Criterion': 'By Average',          'Base': win_mean_gpt.get('Base', 0),   'HyDE': win_mean_gpt.get('HyDE', 0),   'Decomposition': win_mean_gpt.get('Decomposition', 0)},\n",
    "        {'Model': 'GPT',   'Criterion': 'By Statistical Test', 'Base': win_stat_gpt.get('Base', 0),   'HyDE': win_stat_gpt.get('HyDE', 0),   'Decomposition': win_stat_gpt.get('Decomposition', 0)}\n",
    "    ])\n",
    "\n",
    "    summary_table = summary_table.set_index(['Model', 'Criterion'])\n",
    "\n",
    "    print(\"\\n=== Final Summary Table ===\")\n",
    "    print(summary_table)\n",
    "\n",
    "    return summary_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Which strategy achieves the best performance within the Gemma model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AVERAGE OF EACH METRIC BY TECHNIQUE (Gemma) ===\n",
      "tech                   Base  Decomposition    HyDE\n",
      "rouge1-P             0.6209         0.5569  0.5706\n",
      "rouge1-R             0.1888         0.3712  0.2417\n",
      "rouge1-F1            0.2719         0.4336  0.3174\n",
      "rouge2-P             0.2105         0.1499  0.1667\n",
      "rouge2-R             0.0587         0.1019  0.0713\n",
      "rouge2-F1            0.0867         0.1180  0.0941\n",
      "rougeL-P             0.3651         0.2634  0.3134\n",
      "rougeL-R             0.1024         0.1724  0.1256\n",
      "rougeL-F1            0.1506         0.2028  0.1677\n",
      "METEOR               0.1165         0.2099  0.1469\n",
      "BERTScore Precision  0.8810         0.8727  0.8780\n",
      "BERTScore Recall     0.8333         0.8600  0.8421\n",
      "BERTScore F1         0.8562         0.8662  0.8595\n",
      "MATTR                0.7582         0.7579  0.7669\n",
      "\n",
      "=== TECHNIQUE WINNER PER METRIC (Gemma) ===\n",
      "rouge1-P                        Base\n",
      "rouge1-R               Decomposition\n",
      "rouge1-F1              Decomposition\n",
      "rouge2-P                        Base\n",
      "rouge2-R               Decomposition\n",
      "rouge2-F1              Decomposition\n",
      "rougeL-P                        Base\n",
      "rougeL-R               Decomposition\n",
      "rougeL-F1              Decomposition\n",
      "METEOR                 Decomposition\n",
      "BERTScore Precision             Base\n",
      "BERTScore Recall       Decomposition\n",
      "BERTScore F1           Decomposition\n",
      "MATTR                           HyDE\n",
      "dtype: object\n",
      "\n",
      "=== WIN COUNTS (Gemma) ===\n",
      "Decomposition    9\n",
      "Base             4\n",
      "HyDE             1\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "mean_table_gemma, winner_per_metric_gemma, win_counts_gemma = find_strategy_winners_verbose(df_all, \"Gemma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== t-test + Cohen's d Between Techniques (Gemma) ===\n",
      "                 metric         tech_A         tech_B  p_value  cohen_d\n",
      "0              rouge1-P           Base  Decomposition   0.1244     0.54\n",
      "1              rouge1-R           Base  Decomposition   0.0031    -1.26\n",
      "2             rouge1-F1           Base  Decomposition   0.0010    -1.50\n",
      "3              rouge2-P           Base  Decomposition   0.0294     0.82\n",
      "4              rouge2-R           Base  Decomposition   0.0169    -0.93\n",
      "5             rouge2-F1           Base  Decomposition   0.0402    -0.76\n",
      "6              rougeL-P           Base  Decomposition   0.0200     0.89\n",
      "7              rougeL-R           Base  Decomposition   0.0048    -1.18\n",
      "8             rougeL-F1           Base  Decomposition   0.0056    -1.14\n",
      "9                METEOR           Base  Decomposition   0.0059    -1.13\n",
      "10  BERTScore Precision           Base  Decomposition   0.1380     0.51\n",
      "11     BERTScore Recall           Base  Decomposition   0.0112    -1.00\n",
      "12         BERTScore F1           Base  Decomposition   0.0463    -0.73\n",
      "13                MATTR           Base  Decomposition   0.9791     0.01\n",
      "14             rouge1-P           Base           HyDE   0.0546     0.70\n",
      "15             rouge1-R           Base           HyDE   0.2087    -0.43\n",
      "16            rouge1-F1           Base           HyDE   0.2715    -0.37\n",
      "17             rouge2-P           Base           HyDE   0.0457     0.73\n",
      "18             rouge2-R           Base           HyDE   0.3941    -0.28\n",
      "19            rouge2-F1           Base           HyDE   0.6326    -0.16\n",
      "20             rougeL-P           Base           HyDE   0.0731     0.64\n",
      "21             rougeL-R           Base           HyDE   0.2061    -0.43\n",
      "22            rougeL-F1           Base           HyDE   0.2941    -0.35\n",
      "23               METEOR           Base           HyDE   0.2034    -0.43\n",
      "24  BERTScore Precision           Base           HyDE   0.3627     0.30\n",
      "25     BERTScore Recall           Base           HyDE   0.1951    -0.44\n",
      "26         BERTScore F1           Base           HyDE   0.3522    -0.31\n",
      "27                MATTR           Base           HyDE   0.6487    -0.15\n",
      "28             rouge1-P  Decomposition           HyDE   0.6216    -0.16\n",
      "29             rouge1-R  Decomposition           HyDE   0.0304     0.81\n",
      "30            rouge1-F1  Decomposition           HyDE   0.0333     0.79\n",
      "31             rouge2-P  Decomposition           HyDE   0.4899    -0.23\n",
      "32             rouge2-R  Decomposition           HyDE   0.0933     0.59\n",
      "33            rouge2-F1  Decomposition           HyDE   0.1880     0.45\n",
      "34             rougeL-P  Decomposition           HyDE   0.0976    -0.58\n",
      "35             rougeL-R  Decomposition           HyDE   0.0357     0.78\n",
      "36            rougeL-F1  Decomposition           HyDE   0.0873     0.61\n",
      "37               METEOR  Decomposition           HyDE   0.0666     0.66\n",
      "38  BERTScore Precision  Decomposition           HyDE   0.2722    -0.37\n",
      "39     BERTScore Recall  Decomposition           HyDE   0.0663     0.66\n",
      "40         BERTScore F1  Decomposition           HyDE   0.2317     0.41\n",
      "41                MATTR  Decomposition           HyDE   0.6756    -0.14\n"
     ]
    }
   ],
   "source": [
    "stats_df_gemma = ttest_cohen_d_between_techniques(df_all, \"Gemma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Winner per technique (Gemma; p<0.05 & |d|>=0.5) ###\n",
      "winner\n",
      "Decomposition    12\n",
      "Base              3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Decomposition won: rouge1-R, rouge1-F1, rouge2-R, rouge2-F1, rougeL-R, rougeL-F1, METEOR, BERTScore Recall, BERTScore F1, rouge1-R, rouge1-F1, rougeL-R\n",
      "\n",
      "Base won: rouge2-P, rougeL-P, rouge2-P\n"
     ]
    }
   ],
   "source": [
    "sig_gemma, win_counts_gemma = analyze_wins_from_stats(stats_df_gemma, \"Gemma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Which strategy performed best within the GPT model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AVERAGE OF EACH METRIC BY TECHNIQUE (GPT) ===\n",
      "tech                   Base  Decomposition    HyDE\n",
      "rouge1-P             0.3501         0.2742  0.7062\n",
      "rouge1-R             0.6883         0.6756  0.3677\n",
      "rouge1-F1            0.4518         0.3880  0.4746\n",
      "rouge2-P             0.1330         0.0952  0.2778\n",
      "rouge2-R             0.2584         0.2359  0.1464\n",
      "rouge2-F1            0.1705         0.1350  0.1881\n",
      "rougeL-P             0.1784         0.1425  0.3566\n",
      "rougeL-R             0.3562         0.3526  0.1851\n",
      "rougeL-F1            0.2308         0.2019  0.2390\n",
      "METEOR               0.3580         0.3282  0.2220\n",
      "BERTScore Precision  0.8632         0.8527  0.8948\n",
      "BERTScore Recall     0.8925         0.8887  0.8662\n",
      "BERTScore F1         0.8775         0.8703  0.8802\n",
      "MATTR                0.7392         0.7392  0.7457\n",
      "\n",
      "=== TECHNIQUE WINNER PER METRIC (GPT) ===\n",
      "rouge1-P               HyDE\n",
      "rouge1-R               Base\n",
      "rouge1-F1              HyDE\n",
      "rouge2-P               HyDE\n",
      "rouge2-R               Base\n",
      "rouge2-F1              HyDE\n",
      "rougeL-P               HyDE\n",
      "rougeL-R               Base\n",
      "rougeL-F1              HyDE\n",
      "METEOR                 Base\n",
      "BERTScore Precision    HyDE\n",
      "BERTScore Recall       Base\n",
      "BERTScore F1           HyDE\n",
      "MATTR                  HyDE\n",
      "dtype: object\n",
      "\n",
      "=== WIN COUNTS (GPT) ===\n",
      "HyDE    9\n",
      "Base    5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "mean_table_gpt, winner_per_metric_gpt, win_counts_gpt = find_strategy_winners_verbose(df_all, \"GPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== t-test + Cohen's d Between Techniques (GPT) ===\n",
      "                 metric         tech_A         tech_B  p_value  cohen_d\n",
      "0              rouge1-P           Base  Decomposition   0.0394     0.76\n",
      "1              rouge1-R           Base  Decomposition   0.4850     0.23\n",
      "2             rouge1-F1           Base  Decomposition   0.0325     0.80\n",
      "3              rouge2-P           Base  Decomposition   0.0621     0.67\n",
      "4              rouge2-R           Base  Decomposition   0.1416     0.51\n",
      "5             rouge2-F1           Base  Decomposition   0.0541     0.70\n",
      "6              rougeL-P           Base  Decomposition   0.0597     0.68\n",
      "7              rougeL-R           Base  Decomposition   0.7699     0.10\n",
      "8             rougeL-F1           Base  Decomposition   0.0570     0.69\n",
      "9                METEOR           Base  Decomposition   0.0149     0.95\n",
      "10  BERTScore Precision           Base  Decomposition   0.0371     0.77\n",
      "11     BERTScore Recall           Base  Decomposition   0.2032     0.43\n",
      "12         BERTScore F1           Base  Decomposition   0.0408     0.75\n",
      "13                MATTR           Base  Decomposition      NaN      NaN\n",
      "14             rouge1-P           Base           HyDE   0.0000    -2.53\n",
      "15             rouge1-R           Base           HyDE   0.0000     2.52\n",
      "16            rouge1-F1           Base           HyDE   0.1633    -0.48\n",
      "17             rouge2-P           Base           HyDE   0.0001    -2.00\n",
      "18             rouge2-R           Base           HyDE   0.0001     2.10\n",
      "19            rouge2-F1           Base           HyDE   0.1030    -0.57\n",
      "20             rougeL-P           Base           HyDE   0.0000    -2.36\n",
      "21             rougeL-R           Base           HyDE   0.0001     2.05\n",
      "22            rougeL-F1           Base           HyDE   0.3206    -0.33\n",
      "23               METEOR           Base           HyDE   0.0000     2.97\n",
      "24  BERTScore Precision           Base           HyDE   0.0001    -2.14\n",
      "25     BERTScore Recall           Base           HyDE   0.0016     1.41\n",
      "26         BERTScore F1           Base           HyDE   0.1182    -0.55\n",
      "27                MATTR           Base           HyDE   0.5407    -0.20\n",
      "28             rouge1-P  Decomposition           HyDE   0.0000    -4.93\n",
      "29             rouge1-R  Decomposition           HyDE   0.0000     3.06\n",
      "30            rouge1-F1  Decomposition           HyDE   0.0096    -1.04\n",
      "31             rouge2-P  Decomposition           HyDE   0.0001    -2.22\n",
      "32             rouge2-R  Decomposition           HyDE   0.0008     1.55\n",
      "33            rouge2-F1  Decomposition           HyDE   0.0292    -0.82\n",
      "34             rougeL-P  Decomposition           HyDE   0.0000    -3.59\n",
      "35             rougeL-R  Decomposition           HyDE   0.0000     2.65\n",
      "36            rougeL-F1  Decomposition           HyDE   0.0497    -0.72\n",
      "37               METEOR  Decomposition           HyDE   0.0001     2.18\n",
      "38  BERTScore Precision  Decomposition           HyDE   0.0000    -3.47\n",
      "39     BERTScore Recall  Decomposition           HyDE   0.0058     1.14\n",
      "40         BERTScore F1  Decomposition           HyDE   0.0141    -0.96\n",
      "41                MATTR  Decomposition           HyDE   0.5407    -0.20\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mq/3ctzq3zj4w737px9h791vs1r0000gn/T/ipykernel_10668/2332204151.py:18: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return diff.mean() / diff.std(ddof=1)\n"
     ]
    }
   ],
   "source": [
    "stats_df_gpt = ttest_cohen_d_between_techniques(df_all, \"GPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Winner per technique (GPT; p<0.05 & |d|>=0.5) ###\n",
      "winner\n",
      "HyDE             12\n",
      "Base             10\n",
      "Decomposition     5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "HyDE won: rouge1-P, rouge2-P, rougeL-P, BERTScore Precision, rouge1-P, rouge1-F1, rouge2-P, rouge2-F1, rougeL-P, rougeL-F1, BERTScore Precision, BERTScore F1\n",
      "\n",
      "Base won: rouge1-P, rouge1-F1, METEOR, BERTScore Precision, BERTScore F1, rouge1-R, rouge2-R, rougeL-R, METEOR, BERTScore Recall\n",
      "\n",
      "Decomposition won: rouge1-R, rouge2-R, rougeL-R, METEOR, BERTScore Recall\n"
     ]
    }
   ],
   "source": [
    "sig_gpt, win_counts_gpt = analyze_wins_from_stats(stats_df_gpt, \"GPT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Semantic Metric Analysis – Top 3 Techniques (Confidence Level: 95%) ===\n",
      "\n",
      "BERTScore Precision:\n",
      "  GEMMA:\n",
      "    1º: Base: 0.881 [95% CI: 0.876 – 0.886] (n=10)\n",
      "    2º: HyDE: 0.878 [95% CI: 0.872 – 0.884] (n=10)\n",
      "    3º: Decomposition: 0.873 [95% CI: 0.865 – 0.880] (n=10)\n",
      "  GPT:\n",
      "    1º: HyDE: 0.895 [95% CI: 0.888 – 0.901] (n=10)\n",
      "    2º: Base: 0.863 [95% CI: 0.853 – 0.874] (n=10)\n",
      "    3º: Decomposition: 0.853 [95% CI: 0.847 – 0.859] (n=10)\n",
      "\n",
      "BERTScore Recall:\n",
      "  GEMMA:\n",
      "    1º: Decomposition: 0.860 [95% CI: 0.852 – 0.868] (n=10)\n",
      "    2º: HyDE: 0.842 [95% CI: 0.826 – 0.859] (n=10)\n",
      "    3º: Base: 0.833 [95% CI: 0.818 – 0.849] (n=10)\n",
      "  GPT:\n",
      "    1º: Base: 0.892 [95% CI: 0.885 – 0.900] (n=10)\n",
      "    2º: Decomposition: 0.889 [95% CI: 0.883 – 0.895] (n=10)\n",
      "    3º: HyDE: 0.866 [95% CI: 0.855 – 0.877] (n=10)\n",
      "\n",
      "BERTScore F1:\n",
      "  GEMMA:\n",
      "    1º: Decomposition: 0.866 [95% CI: 0.859 – 0.873] (n=10)\n",
      "    2º: HyDE: 0.859 [95% CI: 0.850 – 0.869] (n=10)\n",
      "    3º: Base: 0.856 [95% CI: 0.849 – 0.864] (n=10)\n",
      "  GPT:\n",
      "    1º: HyDE: 0.880 [95% CI: 0.873 – 0.888] (n=10)\n",
      "    2º: Base: 0.878 [95% CI: 0.870 – 0.885] (n=10)\n",
      "    3º: Decomposition: 0.870 [95% CI: 0.866 – 0.875] (n=10)\n",
      "\n",
      "MATTR:\n",
      "  GEMMA:\n",
      "    1º: HyDE: 0.767 [95% CI: 0.732 – 0.802] (n=10)\n",
      "    2º: Base: 0.758 [95% CI: 0.734 – 0.783] (n=10)\n",
      "    3º: Decomposition: 0.758 [95% CI: 0.732 – 0.784] (n=10)\n",
      "  GPT:\n",
      "    1º: HyDE: 0.746 [95% CI: 0.718 – 0.773] (n=10)\n",
      "    2º: Base: 0.739 [95% CI: 0.714 – 0.765] (n=10)\n",
      "    3º: Decomposition: 0.739 [95% CI: 0.714 – 0.765] (n=10)\n",
      "\n",
      "=== Statistical Wins by Metric (p<0.05 and |d|>=0.5) ===\n",
      "\n",
      "BERTScore Precision:\n",
      "  Gemma: no statistically significant wins\n",
      "  GPT  : {'HyDE': 2, 'Base': 1}\n",
      "\n",
      "BERTScore Recall:\n",
      "  Gemma: {'Decomposition': 1}\n",
      "  GPT  : {'Base': 1, 'Decomposition': 1}\n",
      "\n",
      "BERTScore F1:\n",
      "  Gemma: {'Decomposition': 1}\n",
      "  GPT  : {'Base': 1, 'HyDE': 1}\n",
      "\n",
      "MATTR:\n",
      "  Gemma: no statistically significant wins\n",
      "  GPT  : no statistically significant wins\n"
     ]
    }
   ],
   "source": [
    "semantic_metrics = [\n",
    "    'BERTScore Precision',\n",
    "    'BERTScore Recall',\n",
    "    'BERTScore F1',\n",
    "    'MATTR'\n",
    "]\n",
    "\n",
    "analyze_semantic_metrics(\n",
    "    gemma=df_all[df_all['model'] == 'Gemma'],\n",
    "    gpt=df_all[df_all['model'] == 'GPT'],\n",
    "    stat_gemma=sig_gemma,\n",
    "    stat_gpt=sig_gpt,\n",
    "    semantic_metrics=semantic_metrics,\n",
    "    top_k=3,               \n",
    "    confidence_level=0.95 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Which strategy wins within Gemma (by average)?\n",
      "Decomposition    9\n",
      "Base             4\n",
      "HyDE             1\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2) Which strategy wins within GPT (by average)?\n",
      "HyDE    9\n",
      "Base    5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "3) Which strategy wins statistically within Gemma (p<0.05 & |d|>=0.5)?\n",
      "winner\n",
      "Decomposition    12\n",
      "Base              3\n",
      "Name: count, dtype: int64\n",
      "\n",
      "4) Which strategy wins statistically within GPT (p<0.05 & |d|>=0.5)?\n",
      "winner\n",
      "HyDE             12\n",
      "Base             10\n",
      "Decomposition     5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Final Summary Table ===\n",
      "                           Base  HyDE  Decomposition\n",
      "Model Criterion                                     \n",
      "Gemma By Average              4     1              9\n",
      "      By Statistical Test     3     0             12\n",
      "GPT   By Average              5     9              0\n",
      "      By Statistical Test    10    12              5\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/var/folders/mq/3ctzq3zj4w737px9h791vs1r0000gn/T/ipykernel_10668/2702150278.py:56: RuntimeWarning: invalid value encountered in scalar divide\n",
      "  return diff.mean() / diff.std(ddof=1)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Base</th>\n",
       "      <th>HyDE</th>\n",
       "      <th>Decomposition</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th>Criterion</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Gemma</th>\n",
       "      <th>By Average</th>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>By Statistical Test</th>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">GPT</th>\n",
       "      <th>By Average</th>\n",
       "      <td>5</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>By Statistical Test</th>\n",
       "      <td>10</td>\n",
       "      <td>12</td>\n",
       "      <td>5</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Base  HyDE  Decomposition\n",
       "Model Criterion                                     \n",
       "Gemma By Average              4     1              9\n",
       "      By Statistical Test     3     0             12\n",
       "GPT   By Average              5     9              0\n",
       "      By Statistical Test    10    12              5"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(df_all)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## **Engineering**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Importing Dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_base = pd.read_csv('./Validation_Results/df_eng_base.csv')\n",
    "df_eng_decomposition = pd.read_csv('./Validation_Results/df_eng_decomposition.csv')\n",
    "df_eng_hyde = pd.read_csv('./Validation_Results/df_eng_hyde.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_eng_gpt_base = pd.read_csv('./Validation_Results/df_eng_gpt_base.csv')\n",
    "df_eng_gpt_decomposition = pd.read_csv('./Validation_Results/df_eng_gpt_decomposition.csv')\n",
    "df_eng_gpt_hyde = pd.read_csv('./Validation_Results/df_eng_gpt_hyde.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Concat Dataframes**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>rouge1-P</th>\n",
       "      <th>rouge1-R</th>\n",
       "      <th>rouge1-F1</th>\n",
       "      <th>rouge2-P</th>\n",
       "      <th>rouge2-R</th>\n",
       "      <th>rouge2-F1</th>\n",
       "      <th>rougeL-P</th>\n",
       "      <th>rougeL-R</th>\n",
       "      <th>rougeL-F1</th>\n",
       "      <th>METEOR</th>\n",
       "      <th>BERTScore Precision</th>\n",
       "      <th>BERTScore Recall</th>\n",
       "      <th>BERTScore F1</th>\n",
       "      <th>MATTR</th>\n",
       "      <th>model</th>\n",
       "      <th>tech</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.482759</td>\n",
       "      <td>0.370044</td>\n",
       "      <td>0.100719</td>\n",
       "      <td>0.162791</td>\n",
       "      <td>0.124444</td>\n",
       "      <td>0.214286</td>\n",
       "      <td>0.344828</td>\n",
       "      <td>0.264317</td>\n",
       "      <td>0.303169</td>\n",
       "      <td>0.876839</td>\n",
       "      <td>0.887158</td>\n",
       "      <td>0.881968</td>\n",
       "      <td>0.802857</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.258786</td>\n",
       "      <td>0.522581</td>\n",
       "      <td>0.346154</td>\n",
       "      <td>0.057600</td>\n",
       "      <td>0.116505</td>\n",
       "      <td>0.077088</td>\n",
       "      <td>0.124601</td>\n",
       "      <td>0.251613</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.258973</td>\n",
       "      <td>0.856648</td>\n",
       "      <td>0.873663</td>\n",
       "      <td>0.865072</td>\n",
       "      <td>0.806343</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.373874</td>\n",
       "      <td>0.525316</td>\n",
       "      <td>0.436842</td>\n",
       "      <td>0.090498</td>\n",
       "      <td>0.127389</td>\n",
       "      <td>0.105820</td>\n",
       "      <td>0.157658</td>\n",
       "      <td>0.221519</td>\n",
       "      <td>0.184211</td>\n",
       "      <td>0.313936</td>\n",
       "      <td>0.879132</td>\n",
       "      <td>0.884566</td>\n",
       "      <td>0.881840</td>\n",
       "      <td>0.801965</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.289283</td>\n",
       "      <td>0.785558</td>\n",
       "      <td>0.422850</td>\n",
       "      <td>0.112097</td>\n",
       "      <td>0.304825</td>\n",
       "      <td>0.163915</td>\n",
       "      <td>0.120064</td>\n",
       "      <td>0.326039</td>\n",
       "      <td>0.175501</td>\n",
       "      <td>0.362792</td>\n",
       "      <td>0.868069</td>\n",
       "      <td>0.895350</td>\n",
       "      <td>0.881498</td>\n",
       "      <td>0.798523</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.259259</td>\n",
       "      <td>0.753846</td>\n",
       "      <td>0.385827</td>\n",
       "      <td>0.104240</td>\n",
       "      <td>0.304124</td>\n",
       "      <td>0.155263</td>\n",
       "      <td>0.123457</td>\n",
       "      <td>0.358974</td>\n",
       "      <td>0.183727</td>\n",
       "      <td>0.397709</td>\n",
       "      <td>0.862435</td>\n",
       "      <td>0.908050</td>\n",
       "      <td>0.884655</td>\n",
       "      <td>0.815560</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.214844</td>\n",
       "      <td>0.760369</td>\n",
       "      <td>0.335025</td>\n",
       "      <td>0.100391</td>\n",
       "      <td>0.356481</td>\n",
       "      <td>0.156663</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.442396</td>\n",
       "      <td>0.194924</td>\n",
       "      <td>0.366189</td>\n",
       "      <td>0.862882</td>\n",
       "      <td>0.885754</td>\n",
       "      <td>0.874169</td>\n",
       "      <td>0.770629</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0.403651</td>\n",
       "      <td>0.496259</td>\n",
       "      <td>0.445190</td>\n",
       "      <td>0.148374</td>\n",
       "      <td>0.182500</td>\n",
       "      <td>0.163677</td>\n",
       "      <td>0.225152</td>\n",
       "      <td>0.276808</td>\n",
       "      <td>0.248322</td>\n",
       "      <td>0.284623</td>\n",
       "      <td>0.860501</td>\n",
       "      <td>0.862375</td>\n",
       "      <td>0.861437</td>\n",
       "      <td>0.678514</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0.373134</td>\n",
       "      <td>0.724638</td>\n",
       "      <td>0.492611</td>\n",
       "      <td>0.270677</td>\n",
       "      <td>0.529412</td>\n",
       "      <td>0.358209</td>\n",
       "      <td>0.305970</td>\n",
       "      <td>0.594203</td>\n",
       "      <td>0.403941</td>\n",
       "      <td>0.427665</td>\n",
       "      <td>0.897906</td>\n",
       "      <td>0.926077</td>\n",
       "      <td>0.911774</td>\n",
       "      <td>0.687059</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0.257218</td>\n",
       "      <td>0.723247</td>\n",
       "      <td>0.379477</td>\n",
       "      <td>0.094612</td>\n",
       "      <td>0.266667</td>\n",
       "      <td>0.139670</td>\n",
       "      <td>0.139108</td>\n",
       "      <td>0.391144</td>\n",
       "      <td>0.205227</td>\n",
       "      <td>0.345948</td>\n",
       "      <td>0.874368</td>\n",
       "      <td>0.901394</td>\n",
       "      <td>0.887675</td>\n",
       "      <td>0.816101</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0.484487</td>\n",
       "      <td>0.618902</td>\n",
       "      <td>0.543507</td>\n",
       "      <td>0.212919</td>\n",
       "      <td>0.272171</td>\n",
       "      <td>0.238926</td>\n",
       "      <td>0.233890</td>\n",
       "      <td>0.298780</td>\n",
       "      <td>0.262383</td>\n",
       "      <td>0.350207</td>\n",
       "      <td>0.881420</td>\n",
       "      <td>0.882854</td>\n",
       "      <td>0.882136</td>\n",
       "      <td>0.665081</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>0.301948</td>\n",
       "      <td>0.664286</td>\n",
       "      <td>0.415179</td>\n",
       "      <td>0.068404</td>\n",
       "      <td>0.151079</td>\n",
       "      <td>0.094170</td>\n",
       "      <td>0.146104</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.200893</td>\n",
       "      <td>0.342816</td>\n",
       "      <td>0.846273</td>\n",
       "      <td>0.878219</td>\n",
       "      <td>0.861950</td>\n",
       "      <td>0.830656</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>0.497537</td>\n",
       "      <td>0.484026</td>\n",
       "      <td>0.490688</td>\n",
       "      <td>0.125000</td>\n",
       "      <td>0.121600</td>\n",
       "      <td>0.123277</td>\n",
       "      <td>0.167488</td>\n",
       "      <td>0.162939</td>\n",
       "      <td>0.165182</td>\n",
       "      <td>0.264106</td>\n",
       "      <td>0.871230</td>\n",
       "      <td>0.870066</td>\n",
       "      <td>0.870648</td>\n",
       "      <td>0.820464</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>12</th>\n",
       "      <td>0.377119</td>\n",
       "      <td>0.400901</td>\n",
       "      <td>0.388646</td>\n",
       "      <td>0.119149</td>\n",
       "      <td>0.126697</td>\n",
       "      <td>0.122807</td>\n",
       "      <td>0.152542</td>\n",
       "      <td>0.162162</td>\n",
       "      <td>0.157205</td>\n",
       "      <td>0.244459</td>\n",
       "      <td>0.872728</td>\n",
       "      <td>0.871154</td>\n",
       "      <td>0.871940</td>\n",
       "      <td>0.826203</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>13</th>\n",
       "      <td>0.634897</td>\n",
       "      <td>0.348912</td>\n",
       "      <td>0.450338</td>\n",
       "      <td>0.211454</td>\n",
       "      <td>0.116129</td>\n",
       "      <td>0.149922</td>\n",
       "      <td>0.247801</td>\n",
       "      <td>0.136180</td>\n",
       "      <td>0.175767</td>\n",
       "      <td>0.212831</td>\n",
       "      <td>0.878636</td>\n",
       "      <td>0.864066</td>\n",
       "      <td>0.871290</td>\n",
       "      <td>0.813112</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>0.579946</td>\n",
       "      <td>0.377425</td>\n",
       "      <td>0.457265</td>\n",
       "      <td>0.163043</td>\n",
       "      <td>0.106007</td>\n",
       "      <td>0.128480</td>\n",
       "      <td>0.222222</td>\n",
       "      <td>0.144621</td>\n",
       "      <td>0.175214</td>\n",
       "      <td>0.241054</td>\n",
       "      <td>0.869123</td>\n",
       "      <td>0.865256</td>\n",
       "      <td>0.867185</td>\n",
       "      <td>0.834625</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>15</th>\n",
       "      <td>0.680288</td>\n",
       "      <td>0.368490</td>\n",
       "      <td>0.478041</td>\n",
       "      <td>0.243373</td>\n",
       "      <td>0.131682</td>\n",
       "      <td>0.170897</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.145833</td>\n",
       "      <td>0.189189</td>\n",
       "      <td>0.212633</td>\n",
       "      <td>0.885623</td>\n",
       "      <td>0.874923</td>\n",
       "      <td>0.880240</td>\n",
       "      <td>0.789319</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>16</th>\n",
       "      <td>0.515094</td>\n",
       "      <td>0.553753</td>\n",
       "      <td>0.533724</td>\n",
       "      <td>0.217391</td>\n",
       "      <td>0.233740</td>\n",
       "      <td>0.225269</td>\n",
       "      <td>0.232075</td>\n",
       "      <td>0.249493</td>\n",
       "      <td>0.240469</td>\n",
       "      <td>0.325271</td>\n",
       "      <td>0.878051</td>\n",
       "      <td>0.887826</td>\n",
       "      <td>0.882912</td>\n",
       "      <td>0.738960</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17</th>\n",
       "      <td>0.523810</td>\n",
       "      <td>0.574627</td>\n",
       "      <td>0.548043</td>\n",
       "      <td>0.239726</td>\n",
       "      <td>0.263158</td>\n",
       "      <td>0.250896</td>\n",
       "      <td>0.346939</td>\n",
       "      <td>0.380597</td>\n",
       "      <td>0.362989</td>\n",
       "      <td>0.368155</td>\n",
       "      <td>0.903551</td>\n",
       "      <td>0.912525</td>\n",
       "      <td>0.908016</td>\n",
       "      <td>0.842653</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>18</th>\n",
       "      <td>0.714744</td>\n",
       "      <td>0.292651</td>\n",
       "      <td>0.415270</td>\n",
       "      <td>0.228296</td>\n",
       "      <td>0.093298</td>\n",
       "      <td>0.132463</td>\n",
       "      <td>0.323718</td>\n",
       "      <td>0.132546</td>\n",
       "      <td>0.188082</td>\n",
       "      <td>0.186023</td>\n",
       "      <td>0.893771</td>\n",
       "      <td>0.872618</td>\n",
       "      <td>0.883068</td>\n",
       "      <td>0.851407</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>19</th>\n",
       "      <td>0.441729</td>\n",
       "      <td>0.560859</td>\n",
       "      <td>0.494217</td>\n",
       "      <td>0.173258</td>\n",
       "      <td>0.220096</td>\n",
       "      <td>0.193888</td>\n",
       "      <td>0.227444</td>\n",
       "      <td>0.288783</td>\n",
       "      <td>0.254469</td>\n",
       "      <td>0.312077</td>\n",
       "      <td>0.869658</td>\n",
       "      <td>0.884325</td>\n",
       "      <td>0.876930</td>\n",
       "      <td>0.744928</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>20</th>\n",
       "      <td>0.479592</td>\n",
       "      <td>0.335714</td>\n",
       "      <td>0.394958</td>\n",
       "      <td>0.154639</td>\n",
       "      <td>0.107914</td>\n",
       "      <td>0.127119</td>\n",
       "      <td>0.316327</td>\n",
       "      <td>0.221429</td>\n",
       "      <td>0.260504</td>\n",
       "      <td>0.226406</td>\n",
       "      <td>0.885516</td>\n",
       "      <td>0.877796</td>\n",
       "      <td>0.881639</td>\n",
       "      <td>0.835102</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21</th>\n",
       "      <td>0.537383</td>\n",
       "      <td>0.367412</td>\n",
       "      <td>0.436433</td>\n",
       "      <td>0.135831</td>\n",
       "      <td>0.092800</td>\n",
       "      <td>0.110266</td>\n",
       "      <td>0.214953</td>\n",
       "      <td>0.146965</td>\n",
       "      <td>0.174573</td>\n",
       "      <td>0.212641</td>\n",
       "      <td>0.876014</td>\n",
       "      <td>0.869767</td>\n",
       "      <td>0.872880</td>\n",
       "      <td>0.820580</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>22</th>\n",
       "      <td>0.572816</td>\n",
       "      <td>0.265766</td>\n",
       "      <td>0.363077</td>\n",
       "      <td>0.166667</td>\n",
       "      <td>0.076923</td>\n",
       "      <td>0.105263</td>\n",
       "      <td>0.271845</td>\n",
       "      <td>0.126126</td>\n",
       "      <td>0.172308</td>\n",
       "      <td>0.167075</td>\n",
       "      <td>0.887241</td>\n",
       "      <td>0.851970</td>\n",
       "      <td>0.869248</td>\n",
       "      <td>0.826667</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23</th>\n",
       "      <td>0.710027</td>\n",
       "      <td>0.211120</td>\n",
       "      <td>0.325466</td>\n",
       "      <td>0.290761</td>\n",
       "      <td>0.086290</td>\n",
       "      <td>0.133085</td>\n",
       "      <td>0.360434</td>\n",
       "      <td>0.107172</td>\n",
       "      <td>0.165217</td>\n",
       "      <td>0.128403</td>\n",
       "      <td>0.894023</td>\n",
       "      <td>0.862223</td>\n",
       "      <td>0.877835</td>\n",
       "      <td>0.834125</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>24</th>\n",
       "      <td>0.775401</td>\n",
       "      <td>0.255732</td>\n",
       "      <td>0.384615</td>\n",
       "      <td>0.311828</td>\n",
       "      <td>0.102473</td>\n",
       "      <td>0.154255</td>\n",
       "      <td>0.379679</td>\n",
       "      <td>0.125220</td>\n",
       "      <td>0.188329</td>\n",
       "      <td>0.153766</td>\n",
       "      <td>0.906995</td>\n",
       "      <td>0.862831</td>\n",
       "      <td>0.884362</td>\n",
       "      <td>0.827391</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25</th>\n",
       "      <td>0.745098</td>\n",
       "      <td>0.247396</td>\n",
       "      <td>0.371457</td>\n",
       "      <td>0.267717</td>\n",
       "      <td>0.088657</td>\n",
       "      <td>0.133203</td>\n",
       "      <td>0.356863</td>\n",
       "      <td>0.118490</td>\n",
       "      <td>0.177908</td>\n",
       "      <td>0.150314</td>\n",
       "      <td>0.893707</td>\n",
       "      <td>0.868588</td>\n",
       "      <td>0.880969</td>\n",
       "      <td>0.785825</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>26</th>\n",
       "      <td>0.442177</td>\n",
       "      <td>0.395538</td>\n",
       "      <td>0.417559</td>\n",
       "      <td>0.134091</td>\n",
       "      <td>0.119919</td>\n",
       "      <td>0.126609</td>\n",
       "      <td>0.208617</td>\n",
       "      <td>0.186613</td>\n",
       "      <td>0.197002</td>\n",
       "      <td>0.222189</td>\n",
       "      <td>0.861243</td>\n",
       "      <td>0.869184</td>\n",
       "      <td>0.865195</td>\n",
       "      <td>0.729184</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>27</th>\n",
       "      <td>0.577320</td>\n",
       "      <td>0.417910</td>\n",
       "      <td>0.484848</td>\n",
       "      <td>0.187500</td>\n",
       "      <td>0.135338</td>\n",
       "      <td>0.157205</td>\n",
       "      <td>0.340206</td>\n",
       "      <td>0.246269</td>\n",
       "      <td>0.285714</td>\n",
       "      <td>0.271384</td>\n",
       "      <td>0.889502</td>\n",
       "      <td>0.875550</td>\n",
       "      <td>0.882471</td>\n",
       "      <td>0.755000</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>28</th>\n",
       "      <td>0.626598</td>\n",
       "      <td>0.321522</td>\n",
       "      <td>0.424978</td>\n",
       "      <td>0.187179</td>\n",
       "      <td>0.095926</td>\n",
       "      <td>0.126846</td>\n",
       "      <td>0.273657</td>\n",
       "      <td>0.140420</td>\n",
       "      <td>0.185603</td>\n",
       "      <td>0.196471</td>\n",
       "      <td>0.897486</td>\n",
       "      <td>0.885346</td>\n",
       "      <td>0.891375</td>\n",
       "      <td>0.843977</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>29</th>\n",
       "      <td>0.509868</td>\n",
       "      <td>0.369928</td>\n",
       "      <td>0.428769</td>\n",
       "      <td>0.181518</td>\n",
       "      <td>0.131579</td>\n",
       "      <td>0.152566</td>\n",
       "      <td>0.279605</td>\n",
       "      <td>0.202864</td>\n",
       "      <td>0.235131</td>\n",
       "      <td>0.214526</td>\n",
       "      <td>0.865323</td>\n",
       "      <td>0.865963</td>\n",
       "      <td>0.865643</td>\n",
       "      <td>0.756784</td>\n",
       "      <td>Gemma</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>30</th>\n",
       "      <td>0.370370</td>\n",
       "      <td>0.571429</td>\n",
       "      <td>0.449438</td>\n",
       "      <td>0.134884</td>\n",
       "      <td>0.208633</td>\n",
       "      <td>0.163842</td>\n",
       "      <td>0.208333</td>\n",
       "      <td>0.321429</td>\n",
       "      <td>0.252809</td>\n",
       "      <td>0.335823</td>\n",
       "      <td>0.880589</td>\n",
       "      <td>0.889354</td>\n",
       "      <td>0.884950</td>\n",
       "      <td>0.774731</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>31</th>\n",
       "      <td>0.678571</td>\n",
       "      <td>0.303514</td>\n",
       "      <td>0.419426</td>\n",
       "      <td>0.261649</td>\n",
       "      <td>0.116800</td>\n",
       "      <td>0.161504</td>\n",
       "      <td>0.332143</td>\n",
       "      <td>0.148562</td>\n",
       "      <td>0.205298</td>\n",
       "      <td>0.193207</td>\n",
       "      <td>0.898712</td>\n",
       "      <td>0.868497</td>\n",
       "      <td>0.883346</td>\n",
       "      <td>0.841558</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>32</th>\n",
       "      <td>0.658537</td>\n",
       "      <td>0.364865</td>\n",
       "      <td>0.469565</td>\n",
       "      <td>0.270492</td>\n",
       "      <td>0.149321</td>\n",
       "      <td>0.192420</td>\n",
       "      <td>0.382114</td>\n",
       "      <td>0.211712</td>\n",
       "      <td>0.272464</td>\n",
       "      <td>0.290989</td>\n",
       "      <td>0.911879</td>\n",
       "      <td>0.882545</td>\n",
       "      <td>0.896972</td>\n",
       "      <td>0.860811</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>33</th>\n",
       "      <td>0.904564</td>\n",
       "      <td>0.175665</td>\n",
       "      <td>0.294197</td>\n",
       "      <td>0.525000</td>\n",
       "      <td>0.101613</td>\n",
       "      <td>0.170270</td>\n",
       "      <td>0.522822</td>\n",
       "      <td>0.101531</td>\n",
       "      <td>0.170040</td>\n",
       "      <td>0.108201</td>\n",
       "      <td>0.912409</td>\n",
       "      <td>0.855902</td>\n",
       "      <td>0.883252</td>\n",
       "      <td>0.721562</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34</th>\n",
       "      <td>0.691943</td>\n",
       "      <td>0.257496</td>\n",
       "      <td>0.375321</td>\n",
       "      <td>0.238095</td>\n",
       "      <td>0.088339</td>\n",
       "      <td>0.128866</td>\n",
       "      <td>0.331754</td>\n",
       "      <td>0.123457</td>\n",
       "      <td>0.179949</td>\n",
       "      <td>0.160240</td>\n",
       "      <td>0.905563</td>\n",
       "      <td>0.863358</td>\n",
       "      <td>0.883957</td>\n",
       "      <td>0.839630</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>35</th>\n",
       "      <td>0.776398</td>\n",
       "      <td>0.325521</td>\n",
       "      <td>0.458716</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.139505</td>\n",
       "      <td>0.196691</td>\n",
       "      <td>0.419255</td>\n",
       "      <td>0.175781</td>\n",
       "      <td>0.247706</td>\n",
       "      <td>0.201856</td>\n",
       "      <td>0.896572</td>\n",
       "      <td>0.875209</td>\n",
       "      <td>0.885762</td>\n",
       "      <td>0.798974</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36</th>\n",
       "      <td>0.792531</td>\n",
       "      <td>0.387424</td>\n",
       "      <td>0.520436</td>\n",
       "      <td>0.420833</td>\n",
       "      <td>0.205285</td>\n",
       "      <td>0.275956</td>\n",
       "      <td>0.485477</td>\n",
       "      <td>0.237323</td>\n",
       "      <td>0.318801</td>\n",
       "      <td>0.234894</td>\n",
       "      <td>0.903545</td>\n",
       "      <td>0.881117</td>\n",
       "      <td>0.892190</td>\n",
       "      <td>0.639375</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>37</th>\n",
       "      <td>0.492188</td>\n",
       "      <td>0.470149</td>\n",
       "      <td>0.480916</td>\n",
       "      <td>0.196850</td>\n",
       "      <td>0.187970</td>\n",
       "      <td>0.192308</td>\n",
       "      <td>0.281250</td>\n",
       "      <td>0.268657</td>\n",
       "      <td>0.274809</td>\n",
       "      <td>0.297377</td>\n",
       "      <td>0.892109</td>\n",
       "      <td>0.895469</td>\n",
       "      <td>0.893786</td>\n",
       "      <td>0.800759</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>38</th>\n",
       "      <td>0.728324</td>\n",
       "      <td>0.330709</td>\n",
       "      <td>0.454874</td>\n",
       "      <td>0.275362</td>\n",
       "      <td>0.124836</td>\n",
       "      <td>0.171790</td>\n",
       "      <td>0.369942</td>\n",
       "      <td>0.167979</td>\n",
       "      <td>0.231047</td>\n",
       "      <td>0.211322</td>\n",
       "      <td>0.904425</td>\n",
       "      <td>0.883902</td>\n",
       "      <td>0.894046</td>\n",
       "      <td>0.825320</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>39</th>\n",
       "      <td>0.741007</td>\n",
       "      <td>0.491647</td>\n",
       "      <td>0.591105</td>\n",
       "      <td>0.411552</td>\n",
       "      <td>0.272727</td>\n",
       "      <td>0.328058</td>\n",
       "      <td>0.413669</td>\n",
       "      <td>0.274463</td>\n",
       "      <td>0.329986</td>\n",
       "      <td>0.323539</td>\n",
       "      <td>0.905263</td>\n",
       "      <td>0.888730</td>\n",
       "      <td>0.896920</td>\n",
       "      <td>0.660087</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Base</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>40</th>\n",
       "      <td>0.262069</td>\n",
       "      <td>0.542857</td>\n",
       "      <td>0.353488</td>\n",
       "      <td>0.062284</td>\n",
       "      <td>0.129496</td>\n",
       "      <td>0.084112</td>\n",
       "      <td>0.141379</td>\n",
       "      <td>0.292857</td>\n",
       "      <td>0.190698</td>\n",
       "      <td>0.295737</td>\n",
       "      <td>0.856975</td>\n",
       "      <td>0.875580</td>\n",
       "      <td>0.866177</td>\n",
       "      <td>0.803568</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>41</th>\n",
       "      <td>0.765258</td>\n",
       "      <td>0.260383</td>\n",
       "      <td>0.388558</td>\n",
       "      <td>0.278302</td>\n",
       "      <td>0.094400</td>\n",
       "      <td>0.140980</td>\n",
       "      <td>0.394366</td>\n",
       "      <td>0.134185</td>\n",
       "      <td>0.200238</td>\n",
       "      <td>0.164086</td>\n",
       "      <td>0.908239</td>\n",
       "      <td>0.861470</td>\n",
       "      <td>0.884236</td>\n",
       "      <td>0.846707</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>42</th>\n",
       "      <td>0.495455</td>\n",
       "      <td>0.490991</td>\n",
       "      <td>0.493213</td>\n",
       "      <td>0.159817</td>\n",
       "      <td>0.158371</td>\n",
       "      <td>0.159091</td>\n",
       "      <td>0.204545</td>\n",
       "      <td>0.202703</td>\n",
       "      <td>0.203620</td>\n",
       "      <td>0.316554</td>\n",
       "      <td>0.882465</td>\n",
       "      <td>0.883285</td>\n",
       "      <td>0.882875</td>\n",
       "      <td>0.813099</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>43</th>\n",
       "      <td>0.727273</td>\n",
       "      <td>0.199839</td>\n",
       "      <td>0.313527</td>\n",
       "      <td>0.320588</td>\n",
       "      <td>0.087903</td>\n",
       "      <td>0.137975</td>\n",
       "      <td>0.354839</td>\n",
       "      <td>0.097502</td>\n",
       "      <td>0.152971</td>\n",
       "      <td>0.126562</td>\n",
       "      <td>0.877217</td>\n",
       "      <td>0.860419</td>\n",
       "      <td>0.868737</td>\n",
       "      <td>0.765205</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>44</th>\n",
       "      <td>0.753927</td>\n",
       "      <td>0.253968</td>\n",
       "      <td>0.379947</td>\n",
       "      <td>0.300000</td>\n",
       "      <td>0.100707</td>\n",
       "      <td>0.150794</td>\n",
       "      <td>0.397906</td>\n",
       "      <td>0.134039</td>\n",
       "      <td>0.200528</td>\n",
       "      <td>0.166244</td>\n",
       "      <td>0.901800</td>\n",
       "      <td>0.857883</td>\n",
       "      <td>0.879294</td>\n",
       "      <td>0.794507</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>45</th>\n",
       "      <td>0.688564</td>\n",
       "      <td>0.368490</td>\n",
       "      <td>0.480068</td>\n",
       "      <td>0.246341</td>\n",
       "      <td>0.131682</td>\n",
       "      <td>0.171623</td>\n",
       "      <td>0.313869</td>\n",
       "      <td>0.167969</td>\n",
       "      <td>0.218830</td>\n",
       "      <td>0.212576</td>\n",
       "      <td>0.889674</td>\n",
       "      <td>0.872526</td>\n",
       "      <td>0.881016</td>\n",
       "      <td>0.784696</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46</th>\n",
       "      <td>0.813559</td>\n",
       "      <td>0.292089</td>\n",
       "      <td>0.429851</td>\n",
       "      <td>0.511364</td>\n",
       "      <td>0.182927</td>\n",
       "      <td>0.269461</td>\n",
       "      <td>0.446328</td>\n",
       "      <td>0.160243</td>\n",
       "      <td>0.235821</td>\n",
       "      <td>0.200151</td>\n",
       "      <td>0.904826</td>\n",
       "      <td>0.864193</td>\n",
       "      <td>0.884043</td>\n",
       "      <td>0.768281</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>47</th>\n",
       "      <td>0.323944</td>\n",
       "      <td>0.343284</td>\n",
       "      <td>0.333333</td>\n",
       "      <td>0.113475</td>\n",
       "      <td>0.120301</td>\n",
       "      <td>0.116788</td>\n",
       "      <td>0.218310</td>\n",
       "      <td>0.231343</td>\n",
       "      <td>0.224638</td>\n",
       "      <td>0.240675</td>\n",
       "      <td>0.866000</td>\n",
       "      <td>0.879446</td>\n",
       "      <td>0.872671</td>\n",
       "      <td>0.901935</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>48</th>\n",
       "      <td>0.747508</td>\n",
       "      <td>0.295276</td>\n",
       "      <td>0.423330</td>\n",
       "      <td>0.290000</td>\n",
       "      <td>0.114323</td>\n",
       "      <td>0.163996</td>\n",
       "      <td>0.398671</td>\n",
       "      <td>0.157480</td>\n",
       "      <td>0.225776</td>\n",
       "      <td>0.196838</td>\n",
       "      <td>0.908009</td>\n",
       "      <td>0.879094</td>\n",
       "      <td>0.893318</td>\n",
       "      <td>0.794683</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>49</th>\n",
       "      <td>0.704762</td>\n",
       "      <td>0.353222</td>\n",
       "      <td>0.470588</td>\n",
       "      <td>0.392344</td>\n",
       "      <td>0.196172</td>\n",
       "      <td>0.261563</td>\n",
       "      <td>0.452381</td>\n",
       "      <td>0.226730</td>\n",
       "      <td>0.302067</td>\n",
       "      <td>0.249765</td>\n",
       "      <td>0.879876</td>\n",
       "      <td>0.869834</td>\n",
       "      <td>0.874826</td>\n",
       "      <td>0.724472</td>\n",
       "      <td>GPT</td>\n",
       "      <td>Decomposition</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50</th>\n",
       "      <td>0.549296</td>\n",
       "      <td>0.278571</td>\n",
       "      <td>0.369668</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.086331</td>\n",
       "      <td>0.114833</td>\n",
       "      <td>0.338028</td>\n",
       "      <td>0.171429</td>\n",
       "      <td>0.227488</td>\n",
       "      <td>0.202548</td>\n",
       "      <td>0.898465</td>\n",
       "      <td>0.874006</td>\n",
       "      <td>0.886067</td>\n",
       "      <td>0.899091</td>\n",
       "      <td>GPT</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>51</th>\n",
       "      <td>0.851064</td>\n",
       "      <td>0.063898</td>\n",
       "      <td>0.118871</td>\n",
       "      <td>0.326087</td>\n",
       "      <td>0.024000</td>\n",
       "      <td>0.044709</td>\n",
       "      <td>0.595745</td>\n",
       "      <td>0.044728</td>\n",
       "      <td>0.083210</td>\n",
       "      <td>0.044334</td>\n",
       "      <td>0.902908</td>\n",
       "      <td>0.794774</td>\n",
       "      <td>0.845397</td>\n",
       "      <td>0.872340</td>\n",
       "      <td>GPT</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>52</th>\n",
       "      <td>0.755556</td>\n",
       "      <td>0.153153</td>\n",
       "      <td>0.254682</td>\n",
       "      <td>0.318182</td>\n",
       "      <td>0.063348</td>\n",
       "      <td>0.105660</td>\n",
       "      <td>0.488889</td>\n",
       "      <td>0.099099</td>\n",
       "      <td>0.164794</td>\n",
       "      <td>0.105851</td>\n",
       "      <td>0.904096</td>\n",
       "      <td>0.827636</td>\n",
       "      <td>0.864178</td>\n",
       "      <td>0.933333</td>\n",
       "      <td>GPT</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>53</th>\n",
       "      <td>0.947368</td>\n",
       "      <td>0.087027</td>\n",
       "      <td>0.159410</td>\n",
       "      <td>0.530973</td>\n",
       "      <td>0.048387</td>\n",
       "      <td>0.088692</td>\n",
       "      <td>0.701754</td>\n",
       "      <td>0.064464</td>\n",
       "      <td>0.118081</td>\n",
       "      <td>0.056576</td>\n",
       "      <td>0.892473</td>\n",
       "      <td>0.818106</td>\n",
       "      <td>0.853673</td>\n",
       "      <td>0.594462</td>\n",
       "      <td>GPT</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>54</th>\n",
       "      <td>0.763158</td>\n",
       "      <td>0.153439</td>\n",
       "      <td>0.255507</td>\n",
       "      <td>0.256637</td>\n",
       "      <td>0.051237</td>\n",
       "      <td>0.085420</td>\n",
       "      <td>0.412281</td>\n",
       "      <td>0.082892</td>\n",
       "      <td>0.138032</td>\n",
       "      <td>0.102749</td>\n",
       "      <td>0.903104</td>\n",
       "      <td>0.837858</td>\n",
       "      <td>0.869258</td>\n",
       "      <td>0.823385</td>\n",
       "      <td>GPT</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>55</th>\n",
       "      <td>0.920000</td>\n",
       "      <td>0.059896</td>\n",
       "      <td>0.112469</td>\n",
       "      <td>0.510204</td>\n",
       "      <td>0.032595</td>\n",
       "      <td>0.061275</td>\n",
       "      <td>0.640000</td>\n",
       "      <td>0.041667</td>\n",
       "      <td>0.078240</td>\n",
       "      <td>0.037883</td>\n",
       "      <td>0.903617</td>\n",
       "      <td>0.807523</td>\n",
       "      <td>0.852871</td>\n",
       "      <td>0.700000</td>\n",
       "      <td>GPT</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>56</th>\n",
       "      <td>0.886364</td>\n",
       "      <td>0.079108</td>\n",
       "      <td>0.145251</td>\n",
       "      <td>0.627907</td>\n",
       "      <td>0.054878</td>\n",
       "      <td>0.100935</td>\n",
       "      <td>0.818182</td>\n",
       "      <td>0.073022</td>\n",
       "      <td>0.134078</td>\n",
       "      <td>0.046779</td>\n",
       "      <td>0.903180</td>\n",
       "      <td>0.814169</td>\n",
       "      <td>0.856368</td>\n",
       "      <td>0.636364</td>\n",
       "      <td>GPT</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>57</th>\n",
       "      <td>0.708333</td>\n",
       "      <td>0.126866</td>\n",
       "      <td>0.215190</td>\n",
       "      <td>0.347826</td>\n",
       "      <td>0.060150</td>\n",
       "      <td>0.102564</td>\n",
       "      <td>0.666667</td>\n",
       "      <td>0.119403</td>\n",
       "      <td>0.202532</td>\n",
       "      <td>0.094951</td>\n",
       "      <td>0.898708</td>\n",
       "      <td>0.802504</td>\n",
       "      <td>0.847885</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>GPT</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58</th>\n",
       "      <td>0.811321</td>\n",
       "      <td>0.056430</td>\n",
       "      <td>0.105521</td>\n",
       "      <td>0.269231</td>\n",
       "      <td>0.018397</td>\n",
       "      <td>0.034440</td>\n",
       "      <td>0.509434</td>\n",
       "      <td>0.035433</td>\n",
       "      <td>0.066258</td>\n",
       "      <td>0.039750</td>\n",
       "      <td>0.898389</td>\n",
       "      <td>0.809018</td>\n",
       "      <td>0.851365</td>\n",
       "      <td>0.905000</td>\n",
       "      <td>GPT</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>59</th>\n",
       "      <td>0.872340</td>\n",
       "      <td>0.097852</td>\n",
       "      <td>0.175966</td>\n",
       "      <td>0.586957</td>\n",
       "      <td>0.064593</td>\n",
       "      <td>0.116379</td>\n",
       "      <td>0.744681</td>\n",
       "      <td>0.083532</td>\n",
       "      <td>0.150215</td>\n",
       "      <td>0.084787</td>\n",
       "      <td>0.911822</td>\n",
       "      <td>0.815843</td>\n",
       "      <td>0.861167</td>\n",
       "      <td>0.723404</td>\n",
       "      <td>GPT</td>\n",
       "      <td>HyDE</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    rouge1-P  rouge1-R  rouge1-F1  rouge2-P  rouge2-R  rouge2-F1  rougeL-P  \\\n",
       "0   0.300000  0.482759   0.370044  0.100719  0.162791   0.124444  0.214286   \n",
       "1   0.258786  0.522581   0.346154  0.057600  0.116505   0.077088  0.124601   \n",
       "2   0.373874  0.525316   0.436842  0.090498  0.127389   0.105820  0.157658   \n",
       "3   0.289283  0.785558   0.422850  0.112097  0.304825   0.163915  0.120064   \n",
       "4   0.259259  0.753846   0.385827  0.104240  0.304124   0.155263  0.123457   \n",
       "5   0.214844  0.760369   0.335025  0.100391  0.356481   0.156663  0.125000   \n",
       "6   0.403651  0.496259   0.445190  0.148374  0.182500   0.163677  0.225152   \n",
       "7   0.373134  0.724638   0.492611  0.270677  0.529412   0.358209  0.305970   \n",
       "8   0.257218  0.723247   0.379477  0.094612  0.266667   0.139670  0.139108   \n",
       "9   0.484487  0.618902   0.543507  0.212919  0.272171   0.238926  0.233890   \n",
       "10  0.301948  0.664286   0.415179  0.068404  0.151079   0.094170  0.146104   \n",
       "11  0.497537  0.484026   0.490688  0.125000  0.121600   0.123277  0.167488   \n",
       "12  0.377119  0.400901   0.388646  0.119149  0.126697   0.122807  0.152542   \n",
       "13  0.634897  0.348912   0.450338  0.211454  0.116129   0.149922  0.247801   \n",
       "14  0.579946  0.377425   0.457265  0.163043  0.106007   0.128480  0.222222   \n",
       "15  0.680288  0.368490   0.478041  0.243373  0.131682   0.170897  0.269231   \n",
       "16  0.515094  0.553753   0.533724  0.217391  0.233740   0.225269  0.232075   \n",
       "17  0.523810  0.574627   0.548043  0.239726  0.263158   0.250896  0.346939   \n",
       "18  0.714744  0.292651   0.415270  0.228296  0.093298   0.132463  0.323718   \n",
       "19  0.441729  0.560859   0.494217  0.173258  0.220096   0.193888  0.227444   \n",
       "20  0.479592  0.335714   0.394958  0.154639  0.107914   0.127119  0.316327   \n",
       "21  0.537383  0.367412   0.436433  0.135831  0.092800   0.110266  0.214953   \n",
       "22  0.572816  0.265766   0.363077  0.166667  0.076923   0.105263  0.271845   \n",
       "23  0.710027  0.211120   0.325466  0.290761  0.086290   0.133085  0.360434   \n",
       "24  0.775401  0.255732   0.384615  0.311828  0.102473   0.154255  0.379679   \n",
       "25  0.745098  0.247396   0.371457  0.267717  0.088657   0.133203  0.356863   \n",
       "26  0.442177  0.395538   0.417559  0.134091  0.119919   0.126609  0.208617   \n",
       "27  0.577320  0.417910   0.484848  0.187500  0.135338   0.157205  0.340206   \n",
       "28  0.626598  0.321522   0.424978  0.187179  0.095926   0.126846  0.273657   \n",
       "29  0.509868  0.369928   0.428769  0.181518  0.131579   0.152566  0.279605   \n",
       "30  0.370370  0.571429   0.449438  0.134884  0.208633   0.163842  0.208333   \n",
       "31  0.678571  0.303514   0.419426  0.261649  0.116800   0.161504  0.332143   \n",
       "32  0.658537  0.364865   0.469565  0.270492  0.149321   0.192420  0.382114   \n",
       "33  0.904564  0.175665   0.294197  0.525000  0.101613   0.170270  0.522822   \n",
       "34  0.691943  0.257496   0.375321  0.238095  0.088339   0.128866  0.331754   \n",
       "35  0.776398  0.325521   0.458716  0.333333  0.139505   0.196691  0.419255   \n",
       "36  0.792531  0.387424   0.520436  0.420833  0.205285   0.275956  0.485477   \n",
       "37  0.492188  0.470149   0.480916  0.196850  0.187970   0.192308  0.281250   \n",
       "38  0.728324  0.330709   0.454874  0.275362  0.124836   0.171790  0.369942   \n",
       "39  0.741007  0.491647   0.591105  0.411552  0.272727   0.328058  0.413669   \n",
       "40  0.262069  0.542857   0.353488  0.062284  0.129496   0.084112  0.141379   \n",
       "41  0.765258  0.260383   0.388558  0.278302  0.094400   0.140980  0.394366   \n",
       "42  0.495455  0.490991   0.493213  0.159817  0.158371   0.159091  0.204545   \n",
       "43  0.727273  0.199839   0.313527  0.320588  0.087903   0.137975  0.354839   \n",
       "44  0.753927  0.253968   0.379947  0.300000  0.100707   0.150794  0.397906   \n",
       "45  0.688564  0.368490   0.480068  0.246341  0.131682   0.171623  0.313869   \n",
       "46  0.813559  0.292089   0.429851  0.511364  0.182927   0.269461  0.446328   \n",
       "47  0.323944  0.343284   0.333333  0.113475  0.120301   0.116788  0.218310   \n",
       "48  0.747508  0.295276   0.423330  0.290000  0.114323   0.163996  0.398671   \n",
       "49  0.704762  0.353222   0.470588  0.392344  0.196172   0.261563  0.452381   \n",
       "50  0.549296  0.278571   0.369668  0.171429  0.086331   0.114833  0.338028   \n",
       "51  0.851064  0.063898   0.118871  0.326087  0.024000   0.044709  0.595745   \n",
       "52  0.755556  0.153153   0.254682  0.318182  0.063348   0.105660  0.488889   \n",
       "53  0.947368  0.087027   0.159410  0.530973  0.048387   0.088692  0.701754   \n",
       "54  0.763158  0.153439   0.255507  0.256637  0.051237   0.085420  0.412281   \n",
       "55  0.920000  0.059896   0.112469  0.510204  0.032595   0.061275  0.640000   \n",
       "56  0.886364  0.079108   0.145251  0.627907  0.054878   0.100935  0.818182   \n",
       "57  0.708333  0.126866   0.215190  0.347826  0.060150   0.102564  0.666667   \n",
       "58  0.811321  0.056430   0.105521  0.269231  0.018397   0.034440  0.509434   \n",
       "59  0.872340  0.097852   0.175966  0.586957  0.064593   0.116379  0.744681   \n",
       "\n",
       "    rougeL-R  rougeL-F1    METEOR  BERTScore Precision  BERTScore Recall  \\\n",
       "0   0.344828   0.264317  0.303169             0.876839          0.887158   \n",
       "1   0.251613   0.166667  0.258973             0.856648          0.873663   \n",
       "2   0.221519   0.184211  0.313936             0.879132          0.884566   \n",
       "3   0.326039   0.175501  0.362792             0.868069          0.895350   \n",
       "4   0.358974   0.183727  0.397709             0.862435          0.908050   \n",
       "5   0.442396   0.194924  0.366189             0.862882          0.885754   \n",
       "6   0.276808   0.248322  0.284623             0.860501          0.862375   \n",
       "7   0.594203   0.403941  0.427665             0.897906          0.926077   \n",
       "8   0.391144   0.205227  0.345948             0.874368          0.901394   \n",
       "9   0.298780   0.262383  0.350207             0.881420          0.882854   \n",
       "10  0.321429   0.200893  0.342816             0.846273          0.878219   \n",
       "11  0.162939   0.165182  0.264106             0.871230          0.870066   \n",
       "12  0.162162   0.157205  0.244459             0.872728          0.871154   \n",
       "13  0.136180   0.175767  0.212831             0.878636          0.864066   \n",
       "14  0.144621   0.175214  0.241054             0.869123          0.865256   \n",
       "15  0.145833   0.189189  0.212633             0.885623          0.874923   \n",
       "16  0.249493   0.240469  0.325271             0.878051          0.887826   \n",
       "17  0.380597   0.362989  0.368155             0.903551          0.912525   \n",
       "18  0.132546   0.188082  0.186023             0.893771          0.872618   \n",
       "19  0.288783   0.254469  0.312077             0.869658          0.884325   \n",
       "20  0.221429   0.260504  0.226406             0.885516          0.877796   \n",
       "21  0.146965   0.174573  0.212641             0.876014          0.869767   \n",
       "22  0.126126   0.172308  0.167075             0.887241          0.851970   \n",
       "23  0.107172   0.165217  0.128403             0.894023          0.862223   \n",
       "24  0.125220   0.188329  0.153766             0.906995          0.862831   \n",
       "25  0.118490   0.177908  0.150314             0.893707          0.868588   \n",
       "26  0.186613   0.197002  0.222189             0.861243          0.869184   \n",
       "27  0.246269   0.285714  0.271384             0.889502          0.875550   \n",
       "28  0.140420   0.185603  0.196471             0.897486          0.885346   \n",
       "29  0.202864   0.235131  0.214526             0.865323          0.865963   \n",
       "30  0.321429   0.252809  0.335823             0.880589          0.889354   \n",
       "31  0.148562   0.205298  0.193207             0.898712          0.868497   \n",
       "32  0.211712   0.272464  0.290989             0.911879          0.882545   \n",
       "33  0.101531   0.170040  0.108201             0.912409          0.855902   \n",
       "34  0.123457   0.179949  0.160240             0.905563          0.863358   \n",
       "35  0.175781   0.247706  0.201856             0.896572          0.875209   \n",
       "36  0.237323   0.318801  0.234894             0.903545          0.881117   \n",
       "37  0.268657   0.274809  0.297377             0.892109          0.895469   \n",
       "38  0.167979   0.231047  0.211322             0.904425          0.883902   \n",
       "39  0.274463   0.329986  0.323539             0.905263          0.888730   \n",
       "40  0.292857   0.190698  0.295737             0.856975          0.875580   \n",
       "41  0.134185   0.200238  0.164086             0.908239          0.861470   \n",
       "42  0.202703   0.203620  0.316554             0.882465          0.883285   \n",
       "43  0.097502   0.152971  0.126562             0.877217          0.860419   \n",
       "44  0.134039   0.200528  0.166244             0.901800          0.857883   \n",
       "45  0.167969   0.218830  0.212576             0.889674          0.872526   \n",
       "46  0.160243   0.235821  0.200151             0.904826          0.864193   \n",
       "47  0.231343   0.224638  0.240675             0.866000          0.879446   \n",
       "48  0.157480   0.225776  0.196838             0.908009          0.879094   \n",
       "49  0.226730   0.302067  0.249765             0.879876          0.869834   \n",
       "50  0.171429   0.227488  0.202548             0.898465          0.874006   \n",
       "51  0.044728   0.083210  0.044334             0.902908          0.794774   \n",
       "52  0.099099   0.164794  0.105851             0.904096          0.827636   \n",
       "53  0.064464   0.118081  0.056576             0.892473          0.818106   \n",
       "54  0.082892   0.138032  0.102749             0.903104          0.837858   \n",
       "55  0.041667   0.078240  0.037883             0.903617          0.807523   \n",
       "56  0.073022   0.134078  0.046779             0.903180          0.814169   \n",
       "57  0.119403   0.202532  0.094951             0.898708          0.802504   \n",
       "58  0.035433   0.066258  0.039750             0.898389          0.809018   \n",
       "59  0.083532   0.150215  0.084787             0.911822          0.815843   \n",
       "\n",
       "    BERTScore F1     MATTR  model           tech  \n",
       "0       0.881968  0.802857  Gemma           Base  \n",
       "1       0.865072  0.806343  Gemma           Base  \n",
       "2       0.881840  0.801965  Gemma           Base  \n",
       "3       0.881498  0.798523  Gemma           Base  \n",
       "4       0.884655  0.815560  Gemma           Base  \n",
       "5       0.874169  0.770629  Gemma           Base  \n",
       "6       0.861437  0.678514  Gemma           Base  \n",
       "7       0.911774  0.687059  Gemma           Base  \n",
       "8       0.887675  0.816101  Gemma           Base  \n",
       "9       0.882136  0.665081  Gemma           Base  \n",
       "10      0.861950  0.830656  Gemma  Decomposition  \n",
       "11      0.870648  0.820464  Gemma  Decomposition  \n",
       "12      0.871940  0.826203  Gemma  Decomposition  \n",
       "13      0.871290  0.813112  Gemma  Decomposition  \n",
       "14      0.867185  0.834625  Gemma  Decomposition  \n",
       "15      0.880240  0.789319  Gemma  Decomposition  \n",
       "16      0.882912  0.738960  Gemma  Decomposition  \n",
       "17      0.908016  0.842653  Gemma  Decomposition  \n",
       "18      0.883068  0.851407  Gemma  Decomposition  \n",
       "19      0.876930  0.744928  Gemma  Decomposition  \n",
       "20      0.881639  0.835102  Gemma           HyDE  \n",
       "21      0.872880  0.820580  Gemma           HyDE  \n",
       "22      0.869248  0.826667  Gemma           HyDE  \n",
       "23      0.877835  0.834125  Gemma           HyDE  \n",
       "24      0.884362  0.827391  Gemma           HyDE  \n",
       "25      0.880969  0.785825  Gemma           HyDE  \n",
       "26      0.865195  0.729184  Gemma           HyDE  \n",
       "27      0.882471  0.755000  Gemma           HyDE  \n",
       "28      0.891375  0.843977  Gemma           HyDE  \n",
       "29      0.865643  0.756784  Gemma           HyDE  \n",
       "30      0.884950  0.774731    GPT           Base  \n",
       "31      0.883346  0.841558    GPT           Base  \n",
       "32      0.896972  0.860811    GPT           Base  \n",
       "33      0.883252  0.721562    GPT           Base  \n",
       "34      0.883957  0.839630    GPT           Base  \n",
       "35      0.885762  0.798974    GPT           Base  \n",
       "36      0.892190  0.639375    GPT           Base  \n",
       "37      0.893786  0.800759    GPT           Base  \n",
       "38      0.894046  0.825320    GPT           Base  \n",
       "39      0.896920  0.660087    GPT           Base  \n",
       "40      0.866177  0.803568    GPT  Decomposition  \n",
       "41      0.884236  0.846707    GPT  Decomposition  \n",
       "42      0.882875  0.813099    GPT  Decomposition  \n",
       "43      0.868737  0.765205    GPT  Decomposition  \n",
       "44      0.879294  0.794507    GPT  Decomposition  \n",
       "45      0.881016  0.784696    GPT  Decomposition  \n",
       "46      0.884043  0.768281    GPT  Decomposition  \n",
       "47      0.872671  0.901935    GPT  Decomposition  \n",
       "48      0.893318  0.794683    GPT  Decomposition  \n",
       "49      0.874826  0.724472    GPT  Decomposition  \n",
       "50      0.886067  0.899091    GPT           HyDE  \n",
       "51      0.845397  0.872340    GPT           HyDE  \n",
       "52      0.864178  0.933333    GPT           HyDE  \n",
       "53      0.853673  0.594462    GPT           HyDE  \n",
       "54      0.869258  0.823385    GPT           HyDE  \n",
       "55      0.852871  0.700000    GPT           HyDE  \n",
       "56      0.856368  0.636364    GPT           HyDE  \n",
       "57      0.847885  1.000000    GPT           HyDE  \n",
       "58      0.851365  0.905000    GPT           HyDE  \n",
       "59      0.861167  0.723404    GPT           HyDE  "
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_gemma  = pd.concat([\n",
    "    df_eng_base.assign(model='Gemma', tech='Base'),\n",
    "    df_eng_decomposition.assign(model='Gemma', tech='Decomposition'),\n",
    "    df_eng_hyde.assign(model='Gemma', tech='HyDE')\n",
    "])\n",
    "\n",
    "df_gpt = pd.concat([\n",
    "    df_eng_gpt_base.assign(model='GPT', tech='Base'),\n",
    "    df_eng_gpt_decomposition.assign(model='GPT', tech='Decomposition'),\n",
    "    df_eng_gpt_hyde.assign(model='GPT', tech='HyDE')\n",
    "])\n",
    "\n",
    "df_all = pd.concat([df_gemma, df_gpt], ignore_index=True)\n",
    "df_all"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>rouge1-P</th>\n",
       "      <th>rouge1-R</th>\n",
       "      <th>rouge1-F1</th>\n",
       "      <th>rouge2-P</th>\n",
       "      <th>rouge2-R</th>\n",
       "      <th>rouge2-F1</th>\n",
       "      <th>rougeL-P</th>\n",
       "      <th>rougeL-R</th>\n",
       "      <th>rougeL-F1</th>\n",
       "      <th>METEOR</th>\n",
       "      <th>BERTScore Precision</th>\n",
       "      <th>BERTScore Recall</th>\n",
       "      <th>BERTScore F1</th>\n",
       "      <th>MATTR</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>model</th>\n",
       "      <th>tech</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">GPT</th>\n",
       "      <th>Base</th>\n",
       "      <td>0.6834</td>\n",
       "      <td>0.3678</td>\n",
       "      <td>0.4514</td>\n",
       "      <td>0.3068</td>\n",
       "      <td>0.1595</td>\n",
       "      <td>0.1982</td>\n",
       "      <td>0.3747</td>\n",
       "      <td>0.2031</td>\n",
       "      <td>0.2483</td>\n",
       "      <td>0.2357</td>\n",
       "      <td>0.9011</td>\n",
       "      <td>0.8784</td>\n",
       "      <td>0.8895</td>\n",
       "      <td>0.7763</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decomposition</th>\n",
       "      <td>0.6282</td>\n",
       "      <td>0.3400</td>\n",
       "      <td>0.4066</td>\n",
       "      <td>0.2675</td>\n",
       "      <td>0.1316</td>\n",
       "      <td>0.1656</td>\n",
       "      <td>0.3323</td>\n",
       "      <td>0.1805</td>\n",
       "      <td>0.2155</td>\n",
       "      <td>0.2169</td>\n",
       "      <td>0.8875</td>\n",
       "      <td>0.8704</td>\n",
       "      <td>0.8787</td>\n",
       "      <td>0.7997</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HyDE</th>\n",
       "      <td>0.8065</td>\n",
       "      <td>0.1156</td>\n",
       "      <td>0.1913</td>\n",
       "      <td>0.3945</td>\n",
       "      <td>0.0504</td>\n",
       "      <td>0.0855</td>\n",
       "      <td>0.5916</td>\n",
       "      <td>0.0816</td>\n",
       "      <td>0.1363</td>\n",
       "      <td>0.0816</td>\n",
       "      <td>0.9017</td>\n",
       "      <td>0.8201</td>\n",
       "      <td>0.8588</td>\n",
       "      <td>0.8087</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"3\" valign=\"top\">Gemma</th>\n",
       "      <th>Base</th>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.6393</td>\n",
       "      <td>0.4158</td>\n",
       "      <td>0.1292</td>\n",
       "      <td>0.2623</td>\n",
       "      <td>0.1684</td>\n",
       "      <td>0.1769</td>\n",
       "      <td>0.3506</td>\n",
       "      <td>0.2289</td>\n",
       "      <td>0.3411</td>\n",
       "      <td>0.8720</td>\n",
       "      <td>0.8907</td>\n",
       "      <td>0.8812</td>\n",
       "      <td>0.7643</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Decomposition</th>\n",
       "      <td>0.5267</td>\n",
       "      <td>0.4626</td>\n",
       "      <td>0.4671</td>\n",
       "      <td>0.1789</td>\n",
       "      <td>0.1563</td>\n",
       "      <td>0.1592</td>\n",
       "      <td>0.2336</td>\n",
       "      <td>0.2125</td>\n",
       "      <td>0.2109</td>\n",
       "      <td>0.2709</td>\n",
       "      <td>0.8769</td>\n",
       "      <td>0.8781</td>\n",
       "      <td>0.8774</td>\n",
       "      <td>0.8092</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>HyDE</th>\n",
       "      <td>0.5976</td>\n",
       "      <td>0.3188</td>\n",
       "      <td>0.4032</td>\n",
       "      <td>0.2018</td>\n",
       "      <td>0.1038</td>\n",
       "      <td>0.1326</td>\n",
       "      <td>0.3002</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.2042</td>\n",
       "      <td>0.1943</td>\n",
       "      <td>0.8857</td>\n",
       "      <td>0.8689</td>\n",
       "      <td>0.8772</td>\n",
       "      <td>0.8015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                     rouge1-P  rouge1-R  rouge1-F1  rouge2-P  rouge2-R  \\\n",
       "model tech                                                               \n",
       "GPT   Base             0.6834    0.3678     0.4514    0.3068    0.1595   \n",
       "      Decomposition    0.6282    0.3400     0.4066    0.2675    0.1316   \n",
       "      HyDE             0.8065    0.1156     0.1913    0.3945    0.0504   \n",
       "Gemma Base             0.3215    0.6393     0.4158    0.1292    0.2623   \n",
       "      Decomposition    0.5267    0.4626     0.4671    0.1789    0.1563   \n",
       "      HyDE             0.5976    0.3188     0.4032    0.2018    0.1038   \n",
       "\n",
       "                     rouge2-F1  rougeL-P  rougeL-R  rougeL-F1  METEOR  \\\n",
       "model tech                                                              \n",
       "GPT   Base              0.1982    0.3747    0.2031     0.2483  0.2357   \n",
       "      Decomposition     0.1656    0.3323    0.1805     0.2155  0.2169   \n",
       "      HyDE              0.0855    0.5916    0.0816     0.1363  0.0816   \n",
       "Gemma Base              0.1684    0.1769    0.3506     0.2289  0.3411   \n",
       "      Decomposition     0.1592    0.2336    0.2125     0.2109  0.2709   \n",
       "      HyDE              0.1326    0.3002    0.1622     0.2042  0.1943   \n",
       "\n",
       "                     BERTScore Precision  BERTScore Recall  BERTScore F1  \\\n",
       "model tech                                                                 \n",
       "GPT   Base                        0.9011            0.8784        0.8895   \n",
       "      Decomposition               0.8875            0.8704        0.8787   \n",
       "      HyDE                        0.9017            0.8201        0.8588   \n",
       "Gemma Base                        0.8720            0.8907        0.8812   \n",
       "      Decomposition               0.8769            0.8781        0.8774   \n",
       "      HyDE                        0.8857            0.8689        0.8772   \n",
       "\n",
       "                      MATTR  \n",
       "model tech                   \n",
       "GPT   Base           0.7763  \n",
       "      Decomposition  0.7997  \n",
       "      HyDE           0.8087  \n",
       "Gemma Base           0.7643  \n",
       "      Decomposition  0.8092  \n",
       "      HyDE           0.8015  "
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "num_cols = df_all.select_dtypes('number').columns\n",
    "\n",
    "mean_table = (\n",
    "    df_all\n",
    "      .groupby(['model', 'tech'])[num_cols]\n",
    "      .mean()\n",
    "      .round(4)\n",
    ")\n",
    "mean_table"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Which strategy achieves the best performance within the Gemma model?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AVERAGE OF EACH METRIC BY TECHNIQUE (Gemma) ===\n",
      "tech                   Base  Decomposition    HyDE\n",
      "rouge1-P             0.3215         0.5267  0.5976\n",
      "rouge1-R             0.6393         0.4626  0.3188\n",
      "rouge1-F1            0.4158         0.4671  0.4032\n",
      "rouge2-P             0.1292         0.1789  0.2018\n",
      "rouge2-R             0.2623         0.1563  0.1038\n",
      "rouge2-F1            0.1684         0.1592  0.1326\n",
      "rougeL-P             0.1769         0.2336  0.3002\n",
      "rougeL-R             0.3506         0.2125  0.1622\n",
      "rougeL-F1            0.2289         0.2109  0.2042\n",
      "METEOR               0.3411         0.2709  0.1943\n",
      "BERTScore Precision  0.8720         0.8769  0.8857\n",
      "BERTScore Recall     0.8907         0.8781  0.8689\n",
      "BERTScore F1         0.8812         0.8774  0.8772\n",
      "MATTR                0.7643         0.8092  0.8015\n",
      "\n",
      "=== TECHNIQUE WINNER PER METRIC (Gemma) ===\n",
      "rouge1-P                        HyDE\n",
      "rouge1-R                        Base\n",
      "rouge1-F1              Decomposition\n",
      "rouge2-P                        HyDE\n",
      "rouge2-R                        Base\n",
      "rouge2-F1                       Base\n",
      "rougeL-P                        HyDE\n",
      "rougeL-R                        Base\n",
      "rougeL-F1                       Base\n",
      "METEOR                          Base\n",
      "BERTScore Precision             HyDE\n",
      "BERTScore Recall                Base\n",
      "BERTScore F1                    Base\n",
      "MATTR                  Decomposition\n",
      "dtype: object\n",
      "\n",
      "=== WIN COUNTS (Gemma) ===\n",
      "Base             8\n",
      "HyDE             4\n",
      "Decomposition    2\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "mean_table_gemma, winner_per_metric_gemma, win_counts_gemma = find_strategy_winners_verbose(df_all, \"Gemma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== t-test + Cohen's d Between Techniques (Gemma) ===\n",
      "                 metric         tech_A         tech_B  p_value  cohen_d\n",
      "0              rouge1-P           Base  Decomposition   0.0074    -1.09\n",
      "1              rouge1-R           Base  Decomposition   0.0320     0.80\n",
      "2             rouge1-F1           Base  Decomposition   0.0371    -0.77\n",
      "3              rouge2-P           Base  Decomposition   0.0441    -0.74\n",
      "4              rouge2-R           Base  Decomposition   0.0174     0.92\n",
      "5             rouge2-F1           Base  Decomposition   0.5635     0.19\n",
      "6              rougeL-P           Base  Decomposition   0.0512    -0.71\n",
      "7              rougeL-R           Base  Decomposition   0.0028     1.28\n",
      "8             rougeL-F1           Base  Decomposition   0.0209     0.88\n",
      "9                METEOR           Base  Decomposition   0.0237     0.86\n",
      "10  BERTScore Precision           Base  Decomposition   0.3803    -0.29\n",
      "11     BERTScore Recall           Base  Decomposition   0.0656     0.66\n",
      "12         BERTScore F1           Base  Decomposition   0.3525     0.31\n",
      "13                MATTR           Base  Decomposition   0.0108    -1.01\n",
      "14             rouge1-P           Base           HyDE   0.0009    -1.53\n",
      "15             rouge1-R           Base           HyDE   0.0002     1.91\n",
      "16            rouge1-F1           Base           HyDE   0.5644     0.19\n",
      "17             rouge2-P           Base           HyDE   0.0395    -0.76\n",
      "18             rouge2-R           Base           HyDE   0.0020     1.35\n",
      "19            rouge2-F1           Base           HyDE   0.1213     0.54\n",
      "20             rougeL-P           Base           HyDE   0.0024    -1.32\n",
      "21             rougeL-R           Base           HyDE   0.0002     1.90\n",
      "22            rougeL-F1           Base           HyDE   0.0636     0.67\n",
      "23               METEOR           Base           HyDE   0.0001     2.09\n",
      "24  BERTScore Precision           Base           HyDE   0.0451    -0.74\n",
      "25     BERTScore Recall           Base           HyDE   0.0043     1.20\n",
      "26         BERTScore F1           Base           HyDE   0.3083     0.34\n",
      "27                MATTR           Base           HyDE   0.0014    -1.44\n",
      "28             rouge1-P  Decomposition           HyDE   0.0514    -0.71\n",
      "29             rouge1-R  Decomposition           HyDE   0.0006     1.65\n",
      "30            rouge1-F1  Decomposition           HyDE   0.0013     1.46\n",
      "31             rouge2-P  Decomposition           HyDE   0.3334    -0.32\n",
      "32             rouge2-R  Decomposition           HyDE   0.0044     1.19\n",
      "33            rouge2-F1  Decomposition           HyDE   0.0862     0.61\n",
      "34             rougeL-P  Decomposition           HyDE   0.0216    -0.88\n",
      "35             rougeL-R  Decomposition           HyDE   0.0058     1.14\n",
      "36            rougeL-F1  Decomposition           HyDE   0.5761     0.18\n",
      "37               METEOR  Decomposition           HyDE   0.0001     2.12\n",
      "38  BERTScore Precision  Decomposition           HyDE   0.1748    -0.47\n",
      "39     BERTScore Recall  Decomposition           HyDE   0.0697     0.65\n",
      "40         BERTScore F1  Decomposition           HyDE   0.9567     0.02\n",
      "41                MATTR  Decomposition           HyDE   0.4283     0.26\n"
     ]
    }
   ],
   "source": [
    "stats_df_gemma = ttest_cohen_d_between_techniques(df_all, \"Gemma\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Winner per technique (Gemma; p<0.05 & |d|>=0.5) ###\n",
      "winner\n",
      "HyDE             12\n",
      "Base             10\n",
      "Decomposition     5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "HyDE won: rouge1-P, rouge2-P, rougeL-P, BERTScore Precision, rouge1-P, rouge1-F1, rouge2-P, rouge2-F1, rougeL-P, rougeL-F1, BERTScore Precision, BERTScore F1\n",
      "\n",
      "Base won: rouge1-P, rouge1-F1, METEOR, BERTScore Precision, BERTScore F1, rouge1-R, rouge2-R, rougeL-R, METEOR, BERTScore Recall\n",
      "\n",
      "Decomposition won: rouge1-R, rouge2-R, rougeL-R, METEOR, BERTScore Recall\n"
     ]
    }
   ],
   "source": [
    "sig_gpt, win_counts_gpt = analyze_wins_from_stats(stats_df_gpt, \"Gemma\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Which strategy performed best within the GPT model?**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== AVERAGE OF EACH METRIC BY TECHNIQUE (GPT) ===\n",
      "tech                   Base  Decomposition    HyDE\n",
      "rouge1-P             0.6834         0.6282  0.8065\n",
      "rouge1-R             0.3678         0.3400  0.1156\n",
      "rouge1-F1            0.4514         0.4066  0.1913\n",
      "rouge2-P             0.3068         0.2675  0.3945\n",
      "rouge2-R             0.1595         0.1316  0.0504\n",
      "rouge2-F1            0.1982         0.1656  0.0855\n",
      "rougeL-P             0.3747         0.3323  0.5916\n",
      "rougeL-R             0.2031         0.1805  0.0816\n",
      "rougeL-F1            0.2483         0.2155  0.1363\n",
      "METEOR               0.2357         0.2169  0.0816\n",
      "BERTScore Precision  0.9011         0.8875  0.9017\n",
      "BERTScore Recall     0.8784         0.8704  0.8201\n",
      "BERTScore F1         0.8895         0.8787  0.8588\n",
      "MATTR                0.7763         0.7997  0.8087\n",
      "\n",
      "=== TECHNIQUE WINNER PER METRIC (GPT) ===\n",
      "rouge1-P               HyDE\n",
      "rouge1-R               Base\n",
      "rouge1-F1              Base\n",
      "rouge2-P               HyDE\n",
      "rouge2-R               Base\n",
      "rouge2-F1              Base\n",
      "rougeL-P               HyDE\n",
      "rougeL-R               Base\n",
      "rougeL-F1              Base\n",
      "METEOR                 Base\n",
      "BERTScore Precision    HyDE\n",
      "BERTScore Recall       Base\n",
      "BERTScore F1           Base\n",
      "MATTR                  HyDE\n",
      "dtype: object\n",
      "\n",
      "=== WIN COUNTS (GPT) ===\n",
      "Base    9\n",
      "HyDE    5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "mean_table_gpt, winner_per_metric_gpt, win_counts_gpt = find_strategy_winners_verbose(df_all, \"GPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== t-test + Cohen's d Between Techniques (GPT) ===\n",
      "                 metric         tech_A         tech_B  p_value  cohen_d\n",
      "0              rouge1-P           Base  Decomposition   0.1126     0.56\n",
      "1              rouge1-R           Base  Decomposition   0.3054     0.34\n",
      "2             rouge1-F1           Base  Decomposition   0.0544     0.70\n",
      "3              rouge2-P           Base  Decomposition   0.1953     0.44\n",
      "4              rouge2-R           Base  Decomposition   0.0298     0.81\n",
      "5             rouge2-F1           Base  Decomposition   0.0120     0.99\n",
      "6              rougeL-P           Base  Decomposition   0.1717     0.47\n",
      "7              rougeL-R           Base  Decomposition   0.0212     0.88\n",
      "8             rougeL-F1           Base  Decomposition   0.0114     1.00\n",
      "9                METEOR           Base  Decomposition   0.1097     0.56\n",
      "10  BERTScore Precision           Base  Decomposition   0.0249     0.85\n",
      "11     BERTScore Recall           Base  Decomposition   0.0112     1.01\n",
      "12         BERTScore F1           Base  Decomposition   0.0029     1.28\n",
      "13                MATTR           Base  Decomposition   0.2569    -0.38\n",
      "14             rouge1-P           Base           HyDE   0.0001    -2.25\n",
      "15             rouge1-R           Base           HyDE   0.0000     2.61\n",
      "16            rouge1-F1           Base           HyDE   0.0001     2.21\n",
      "17             rouge2-P           Base           HyDE   0.0075    -1.08\n",
      "18             rouge2-R           Base           HyDE   0.0001     2.25\n",
      "19            rouge2-F1           Base           HyDE   0.0001     2.10\n",
      "20             rougeL-P           Base           HyDE   0.0001    -2.03\n",
      "21             rougeL-R           Base           HyDE   0.0000     2.43\n",
      "22            rougeL-F1           Base           HyDE   0.0003     1.83\n",
      "23               METEOR           Base           HyDE   0.0000     2.57\n",
      "24  BERTScore Precision           Base           HyDE   0.8662    -0.05\n",
      "25     BERTScore Recall           Base           HyDE   0.0000     2.37\n",
      "26         BERTScore F1           Base           HyDE   0.0001     2.20\n",
      "27                MATTR           Base           HyDE   0.3248    -0.33\n",
      "28             rouge1-P  Decomposition           HyDE   0.0011    -1.50\n",
      "29             rouge1-R  Decomposition           HyDE   0.0000     2.97\n",
      "30            rouge1-F1  Decomposition           HyDE   0.0003     1.84\n",
      "31             rouge2-P  Decomposition           HyDE   0.0042    -1.20\n",
      "32             rouge2-R  Decomposition           HyDE   0.0000     2.41\n",
      "33            rouge2-F1  Decomposition           HyDE   0.0026     1.30\n",
      "34             rougeL-P  Decomposition           HyDE   0.0001    -2.00\n",
      "35             rougeL-R  Decomposition           HyDE   0.0000     2.86\n",
      "36            rougeL-F1  Decomposition           HyDE   0.0039     1.22\n",
      "37               METEOR  Decomposition           HyDE   0.0000     2.84\n",
      "38  BERTScore Precision  Decomposition           HyDE   0.0323    -0.80\n",
      "39     BERTScore Recall  Decomposition           HyDE   0.0001     2.13\n",
      "40         BERTScore F1  Decomposition           HyDE   0.0057     1.14\n",
      "41                MATTR  Decomposition           HyDE   0.7925    -0.09\n"
     ]
    }
   ],
   "source": [
    "stats_df_gpt = ttest_cohen_d_between_techniques(df_all, \"GPT\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "### Winner per technique (GPT; p<0.05 & |d|>=0.5) ###\n",
      "winner\n",
      "Base             16\n",
      "Decomposition     9\n",
      "HyDE              7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Base won: rouge2-R, rouge2-F1, rougeL-R, rougeL-F1, BERTScore Precision, BERTScore Recall, BERTScore F1, rouge1-R, rouge1-F1, rouge2-R, rouge2-F1, rougeL-R, rougeL-F1, METEOR, BERTScore Recall, BERTScore F1\n",
      "\n",
      "Decomposition won: rouge1-R, rouge1-F1, rouge2-R, rouge2-F1, rougeL-R, rougeL-F1, METEOR, BERTScore Recall, BERTScore F1\n",
      "\n",
      "HyDE won: rouge1-P, rouge2-P, rougeL-P, rouge1-P, rouge2-P, rougeL-P, BERTScore Precision\n"
     ]
    }
   ],
   "source": [
    "sig_gpt, win_counts_gpt = analyze_wins_from_stats(stats_df_gpt, \"GPT\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Overview**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "=== Semantic Metric Analysis – Top 3 Techniques (Confidence Level: 95%) ===\n",
      "\n",
      "BERTScore Precision:\n",
      "  GEMMA:\n",
      "    1º: HyDE: 0.886 [95% CI: 0.877 – 0.895] (n=10)\n",
      "    2º: Decomposition: 0.877 [95% CI: 0.867 – 0.887] (n=10)\n",
      "    3º: Base: 0.872 [95% CI: 0.864 – 0.880] (n=10)\n",
      "  GPT:\n",
      "    1º: HyDE: 0.902 [95% CI: 0.899 – 0.905] (n=10)\n",
      "    2º: Base: 0.901 [95% CI: 0.895 – 0.907] (n=10)\n",
      "    3º: Decomposition: 0.888 [95% CI: 0.876 – 0.899] (n=10)\n",
      "\n",
      "BERTScore Recall:\n",
      "  GEMMA:\n",
      "    1º: Base: 0.891 [95% CI: 0.880 – 0.902] (n=10)\n",
      "    2º: Decomposition: 0.878 [95% CI: 0.869 – 0.887] (n=10)\n",
      "    3º: HyDE: 0.869 [95% CI: 0.863 – 0.875] (n=10)\n",
      "  GPT:\n",
      "    1º: Base: 0.878 [95% CI: 0.871 – 0.886] (n=10)\n",
      "    2º: Decomposition: 0.870 [95% CI: 0.865 – 0.876] (n=10)\n",
      "    3º: HyDE: 0.820 [95% CI: 0.806 – 0.834] (n=10)\n",
      "\n",
      "BERTScore F1:\n",
      "  GEMMA:\n",
      "    1º: Base: 0.881 [95% CI: 0.873 – 0.890] (n=10)\n",
      "    2º: Decomposition: 0.877 [95% CI: 0.870 – 0.885] (n=10)\n",
      "    3º: HyDE: 0.877 [95% CI: 0.872 – 0.883] (n=10)\n",
      "  GPT:\n",
      "    1º: Base: 0.890 [95% CI: 0.886 – 0.893] (n=10)\n",
      "    2º: Decomposition: 0.879 [95% CI: 0.874 – 0.884] (n=10)\n",
      "    3º: HyDE: 0.859 [95% CI: 0.851 – 0.866] (n=10)\n",
      "\n",
      "MATTR:\n",
      "  GEMMA:\n",
      "    1º: Decomposition: 0.809 [95% CI: 0.785 – 0.834] (n=10)\n",
      "    2º: HyDE: 0.801 [95% CI: 0.776 – 0.827] (n=10)\n",
      "    3º: Base: 0.764 [95% CI: 0.726 – 0.803] (n=10)\n",
      "  GPT:\n",
      "    1º: HyDE: 0.809 [95% CI: 0.724 – 0.894] (n=10)\n",
      "    2º: Decomposition: 0.800 [95% CI: 0.770 – 0.830] (n=10)\n",
      "    3º: Base: 0.776 [95% CI: 0.728 – 0.824] (n=10)\n",
      "\n",
      "=== Statistical Wins by Metric (p<0.05 and |d|>=0.5) ===\n",
      "\n",
      "BERTScore Precision:\n",
      "  Gemma: no statistically significant wins\n",
      "  GPT  : {'Base': 1, 'HyDE': 1}\n",
      "\n",
      "BERTScore Recall:\n",
      "  Gemma: {'Decomposition': 1}\n",
      "  GPT  : {'Base': 2, 'Decomposition': 1}\n",
      "\n",
      "BERTScore F1:\n",
      "  Gemma: {'Decomposition': 1}\n",
      "  GPT  : {'Base': 2, 'Decomposition': 1}\n",
      "\n",
      "MATTR:\n",
      "  Gemma: no statistically significant wins\n",
      "  GPT  : no statistically significant wins\n"
     ]
    }
   ],
   "source": [
    "semantic_metrics = [\n",
    "    'BERTScore Precision',\n",
    "    'BERTScore Recall',\n",
    "    'BERTScore F1',\n",
    "    'MATTR'\n",
    "]\n",
    "\n",
    "analyze_semantic_metrics(\n",
    "    gemma=df_all[df_all['model'] == 'Gemma'],\n",
    "    gpt=df_all[df_all['model'] == 'GPT'],\n",
    "    stat_gemma=sig_gemma,\n",
    "    stat_gpt=sig_gpt,\n",
    "    semantic_metrics=semantic_metrics,\n",
    "    top_k=3,               \n",
    "    confidence_level=0.95 \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1) Which strategy wins within Gemma (by average)?\n",
      "Base             8\n",
      "HyDE             4\n",
      "Decomposition    2\n",
      "Name: count, dtype: int64\n",
      "\n",
      "2) Which strategy wins within GPT (by average)?\n",
      "Base    9\n",
      "HyDE    5\n",
      "Name: count, dtype: int64\n",
      "\n",
      "3) Which strategy wins statistically within Gemma (p<0.05 & |d|>=0.5)?\n",
      "winner\n",
      "Base             10\n",
      "Decomposition     9\n",
      "HyDE              6\n",
      "Name: count, dtype: int64\n",
      "\n",
      "4) Which strategy wins statistically within GPT (p<0.05 & |d|>=0.5)?\n",
      "winner\n",
      "Base             16\n",
      "Decomposition     9\n",
      "HyDE              7\n",
      "Name: count, dtype: int64\n",
      "\n",
      "=== Final Summary Table ===\n",
      "                           Base  HyDE  Decomposition\n",
      "Model Criterion                                     \n",
      "Gemma By Average              8     4              2\n",
      "      By Statistical Test    10     6              9\n",
      "GPT   By Average              9     5              0\n",
      "      By Statistical Test    16     7              9\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th>Base</th>\n",
       "      <th>HyDE</th>\n",
       "      <th>Decomposition</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>Model</th>\n",
       "      <th>Criterion</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">Gemma</th>\n",
       "      <th>By Average</th>\n",
       "      <td>8</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>By Statistical Test</th>\n",
       "      <td>10</td>\n",
       "      <td>6</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th rowspan=\"2\" valign=\"top\">GPT</th>\n",
       "      <th>By Average</th>\n",
       "      <td>9</td>\n",
       "      <td>5</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>By Statistical Test</th>\n",
       "      <td>16</td>\n",
       "      <td>7</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                           Base  HyDE  Decomposition\n",
       "Model Criterion                                     \n",
       "Gemma By Average              8     4              2\n",
       "      By Statistical Test    10     6              9\n",
       "GPT   By Average              9     5              0\n",
       "      By Statistical Test    16     7              9"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary(df_all)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
